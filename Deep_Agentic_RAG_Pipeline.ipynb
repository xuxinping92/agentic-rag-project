{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40285711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Configuration Dictionary to manage all system parameters\n",
    "config = {\n",
    "    \"data_dir\": \"./data\", # Directory to store raw and cleaned data\n",
    "    \"vector_store_dir\": \"./vector_store\", # Directory to persist our vector store\n",
    "    \"llm_provider\": \"openai\", # The LLM provider we are using\n",
    "    \"reasoning_llm\":\"gpt-4o\", # The powerful model for planning and synthesis\n",
    "    \"fast_llm\": \"gpt-4o-mini\", # A faster, cheaper model for simpler tasks like the baseline RAG\n",
    "    \"embedding_model\": \"text-embedding-3-small\", # The model for creating document embeddings\n",
    "    \"reranker_model\":\"cross-encoder/ms-marco-MiniLM-L-6-v2\", # The model for precision reranking\n",
    "    \"max_reasoning_iterations\": 7, # A safeguard to prevent the agent from getting into an infinite loop\n",
    "    \"top_k_retrieval\": 10, # Number of documents for initial broad recall\n",
    "    \"top_n_rerank\": 3, # Number of documents to keep after precision reranking\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c93c5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # For interacting with the operating system (e.g., managing environment variables)\n",
    "import re # For regular expression operations, useful for text cleaning\n",
    "import json # For working with JSON data\n",
    "from getpass import getpass # To securely prompt for user input like API keys without echoing to the screen\n",
    "from pprint import pprint # For pretty-printing Python objects, making them more readable\n",
    "import uuid # To generate unique identifiers\n",
    "from typing import List, Dict, TypedDict, Literal, Optional # For type hinting to create clean, readable, and maintainable code\n",
    "\n",
    "# Helper function to securely set environment variables if they are not already present\n",
    "def _set_env(var: str):\n",
    "    # Check if the environment variable is not already set\n",
    "    if not os.environ.get(var): # If not, prompt the user to enter it securely\n",
    "        os.environ[var] = getpass(f\"Enter your{var}: \")\n",
    "\n",
    "# Set the API keys for the services we will use\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# For accessing OpenAI models (GPT-4o, embeddings)\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "# For tracing and debugging with LangSmith\n",
    "_set_env(\"TAVILY_API_KEY\") # For the web search tool\n",
    "# Enable LangSmith tracing to get detailed logs and visualizations of our agent's execution\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# Define a project name in LangSmith to organize our runs\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"Advanced-Deep-Thinking-RAG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0579d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_local_10k(html_path, clean_path):\n",
    "    \"\"\"\n",
    "    Parse a locally downloaded SEC 10-K iXBRL HTML file and extract cleaned human-readable text.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Parsing local 10-K file: {html_path}\")\n",
    "\n",
    "    with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # 1) 去掉明显不需要的标签\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # 2) 去掉带命名空间的标签 (iXBRL: ix:, xbrli:, us-gaap: 等)\n",
    "    #    BeautifulSoup 会把它们当成名字里带冒号的 tag\n",
    "    for tag in soup.find_all():\n",
    "        if \":\" in tag.name:\n",
    "            tag.decompose()\n",
    "\n",
    "    # 3) 从常见容器中抽取文本\n",
    "    text_parts = []\n",
    "    for tag in soup.find_all([\"p\", \"div\", \"li\", \"td\"]):\n",
    "        t = tag.get_text(\" \", strip=True)\n",
    "        if not t:\n",
    "            continue\n",
    "\n",
    "        # 过滤明显是 XBRL / 垃圾行\n",
    "        if any(bad in t for bad in [\"us-gaap:\", \"xbrli:\", \"iso4217:\", \"dei:\"]):\n",
    "            continue\n",
    "        if len(t) < 5:\n",
    "            continue\n",
    "\n",
    "        text_parts.append(t)\n",
    "\n",
    "    # 4) 合并 & 清洗\n",
    "    text = \"\\n\\n\".join(text_parts)\n",
    "\n",
    "    # 连续空行压缩\n",
    "    clean_text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    # 多空格压缩\n",
    "    clean_text = re.sub(r\"[ \\t]{2,}\", \" \", clean_text).strip()\n",
    "\n",
    "    os.makedirs(os.path.dirname(clean_path), exist_ok=True)\n",
    "    with open(clean_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(clean_text)\n",
    "\n",
    "    print(f\"✅ Cleaned text saved to: {clean_path}\")\n",
    "    print(\"\\n--- Preview ---\\n\")\n",
    "    print(clean_text[:1000] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4c7e8574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录： c:\\Users\\22959\\Documents\\WXSK_Works\\Agentic RAG Project\n",
      "原始文件路径： c:\\Users\\22959\\Documents\\WXSK_Works\\Agentic RAG Project\\data\\nvda_10k_raw.html\n",
      "清洗文件路径： c:\\Users\\22959\\Documents\\WXSK_Works\\Agentic RAG Project\\data\\nvda_10k_clean.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 定义数据文件夹\n",
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "os.makedirs(data_dir, exist_ok=True)  # 如果不存在则自动创建\n",
    "\n",
    "# 三个变量定义\n",
    "url_10k = \"https://www.sec.gov/Archives/edgar/data/1045810/000104581023000017/nvda-20230129.htm\"\n",
    "doc_path_raw = os.path.join(data_dir, \"nvda_10k_raw.html\")\n",
    "doc_path_clean = os.path.join(data_dir, \"nvda_10k_clean.txt\")\n",
    "\n",
    "print(\"当前工作目录：\", os.getcwd())\n",
    "print(\"原始文件路径：\", doc_path_raw)\n",
    "print(\"清洗文件路径：\", doc_path_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d54cf601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing local 10-K file: c:\\Users\\22959\\Documents\\WXSK_Works\\Agentic RAG Project\\data\\nvda_20230129.htm\n",
      "✅ Cleaned text saved to: c:\\Users\\22959\\Documents\\WXSK_Works\\Agentic RAG Project\\data\\nvda_10k_clean.txt\n",
      "\n",
      "--- Preview ---\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "UNITED STATES\n",
      "\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "\n",
      "Washington, D.C. 20549\n",
      "\n",
      "____________________________________________________________________________________________\n",
      "\n",
      "ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "\n",
      "ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "\n",
      "For the fiscal year ended\n",
      "\n",
      "TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "\n",
      "TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "\n",
      "Commission file number:\n",
      "\n",
      "ORATION\n",
      "\n",
      "(Exact name of registrant as specified in its charter) (State or other jurisdiction of (I.R.S. Employer Incorporation or Organization) Identification No.)\n",
      "\n",
      "(State or other jurisdiction of\n",
      "\n",
      "(I.R.S. Employer\n",
      "\n",
      "Incorporation or Organization)\n",
      "\n",
      "Identification No.)\n",
      "\n",
      "(Address, including zip code, and telephone number, including area code, of principal executive offices)\n",
      "\n",
      "Securities reg...\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "html_path = os.path.join(data_dir, \"nvda_20230129.htm\")\n",
    "clean_path = os.path.join(data_dir, \"nvda_10k_clean.txt\")\n",
    "\n",
    "parse_local_10k(html_path, clean_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "632b4334",
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_query = (\n",
    "    \"Based on NVIDIA's 2023 10-K filing, identify their key risks related to competition. \"\n",
    "    \"Then, find recent news (post-filing, from 2024) about AMD's AI chip strategy and explain \"\n",
    "    \"how this new strategy directly addresses or exacerbates one of NVIDIA's stated risks.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "70564000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and chunking the document...\n",
      "Document loaded and split into 359 chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader  # 文本加载器\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # 文本切分器\n",
    "\n",
    "\n",
    "print(\"Loading and chunking the document...\")\n",
    "\n",
    "# Initialize the loader with the path to our cleaned 10-K file\n",
    "loader = TextLoader(doc_path_clean, encoding='utf-8')\n",
    "\n",
    "# Load the document into memory\n",
    "documents = loader.load()\n",
    "\n",
    "# Initialize the text splitter with a defined chunk size and overlap\n",
    "# chunk_size=1000: Each chunk will be approximately 1000 characters long.\n",
    "# chunk_overlap=150: Each chunk will share 150 characters with the previous one to maintain context.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150\n",
    ")\n",
    "\n",
    "# Split the loaded document into smaller, manageable chunks\n",
    "doc_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Document loaded and split into {len(doc_chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "96f302c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating baseline vector store...\n",
      "Vector store created with 1109 embeddings.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma  # The vector store we will use\n",
    "from langchain_openai import OpenAIEmbeddings  # The function to create embeddings\n",
    "\n",
    "print(\"Creating baseline vector store...\")\n",
    "\n",
    "# Initialize the embedding function using the model specified in our config\n",
    "embedding_function = OpenAIEmbeddings(model=config['embedding_model'])\n",
    "\n",
    "# Create the Chroma vector store from our document chunks\n",
    "# This process takes each chunk, creates an embedding for it, and indexes it.\n",
    "baseline_vector_store = Chroma.from_documents(\n",
    "    documents=doc_chunks,\n",
    "    embedding=embedding_function\n",
    ")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "# The retriever is the component that will actually perform the search.\n",
    "# search_kwargs={\"k\": 3}: This tells the retriever to return the top 3 most relevant chunks for any given query.\n",
    "baseline_retriever = baseline_vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(f\"Vector store created with {baseline_vector_store._collection.count()} embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4daca5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate  # For creating prompt templates\n",
    "from langchain_openai import ChatOpenAI  # The OpenAI chat model interface\n",
    "from langchain_core.runnables import RunnablePassthrough  # A tool to pass inputs through the chain\n",
    "from langchain_core.output_parsers import StrOutputParser  # To parse the LLM's output as a simple string\n",
    "\n",
    "# This template instructs the LLM on how to behave.\n",
    "# {context}: This is where we will inject the content from our retrieved documents.\n",
    "# {question}: This is where the user's original question will go.\n",
    "template = \"\"\"\n",
    "You are an AI financial analyst. Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt template from the text\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# We use our 'fast_llm' for this simple task, as defined in our config\n",
    "llm = ChatOpenAI(model=config[\"fast_llm\"], temperature=0)\n",
    "\n",
    "# A helper function to format the list of retrieved documents into a single string\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n---\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# The complete RAG chain defined using LCEL's pipe (|) syntax\n",
    "baseline_rag_chain = (\n",
    "    {\n",
    "        \"context\": baseline_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    # The context is generated by taking the question, passing it to the retriever, and formatting the result\n",
    "    # The original question is passed through unchanged\n",
    "    | prompt  # The dictionary is then passed to the prompt template\n",
    "    | llm  # The formatted prompt is passed to the language model\n",
    "    | StrOutputParser()  # The LLM's output message is parsed into a string\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2b907d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing complex query on the baseline RAG chain...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--- BASELINE RAG FAILED OUTPUT ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--- BASELINE RAG FAILED OUTPUT ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Based on NVIDIA's 2023 10-K filing, the key risks related to competition include the potential failure to meet the \n",
       "evolving needs of their industry and markets. This suggests that if NVIDIA does not keep pace with technological   \n",
       "advancements or shifts in consumer demand, it could adversely impact their financial results and market position.  \n",
       "\n",
       "Regarding AMD's AI chip strategy, while I cannot access real-time news or updates post-2023, I can provide a       \n",
       "hypothetical analysis based on typical competitive dynamics in the semiconductor industry. If AMD were to announce \n",
       "a new AI chip strategy that focuses on developing advanced AI processing capabilities, this could directly address \n",
       "NVIDIA's risk of failing to meet industry needs.                                                                   \n",
       "\n",
       "For instance, if AMD's new AI chips offer superior performance, efficiency, or cost-effectiveness compared to      \n",
       "NVIDIA's offerings, it could attract customers away from NVIDIA, thereby exacerbating the risk of declining        \n",
       "financial results for NVIDIA. This competitive pressure could force NVIDIA to accelerate its innovation and adapt  \n",
       "its product offerings to maintain its market share, highlighting the importance of staying ahead in a rapidly      \n",
       "evolving industry.                                                                                                 \n",
       "\n",
       "In summary, AMD's advancements in AI chip technology could intensify competition and challenge NVIDIA's ability to \n",
       "meet market demands, directly impacting their financial performance and stock price.                               \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Based on NVIDIA's 2023 10-K filing, the key risks related to competition include the potential failure to meet the \n",
       "evolving needs of their industry and markets. This suggests that if NVIDIA does not keep pace with technological   \n",
       "advancements or shifts in consumer demand, it could adversely impact their financial results and market position.  \n",
       "\n",
       "Regarding AMD's AI chip strategy, while I cannot access real-time news or updates post-2023, I can provide a       \n",
       "hypothetical analysis based on typical competitive dynamics in the semiconductor industry. If AMD were to announce \n",
       "a new AI chip strategy that focuses on developing advanced AI processing capabilities, this could directly address \n",
       "NVIDIA's risk of failing to meet industry needs.                                                                   \n",
       "\n",
       "For instance, if AMD's new AI chips offer superior performance, efficiency, or cost-effectiveness compared to      \n",
       "NVIDIA's offerings, it could attract customers away from NVIDIA, thereby exacerbating the risk of declining        \n",
       "financial results for NVIDIA. This competitive pressure could force NVIDIA to accelerate its innovation and adapt  \n",
       "its product offerings to maintain its market share, highlighting the importance of staying ahead in a rapidly      \n",
       "evolving industry.                                                                                                 \n",
       "\n",
       "In summary, AMD's advancements in AI chip technology could intensify competition and challenge NVIDIA's ability to \n",
       "meet market demands, directly impacting their financial performance and stock price.                               \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console  # For pretty-printing output with markdown\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# Initialize the rich console for better output formatting\n",
    "console = Console()\n",
    "\n",
    "# Our complex, multi-hop, multi-source query\n",
    "complex_query_adv = (\n",
    "    \"Based on NVIDIA's 2023 10-K filing, identify their key risks related to competition. \"\n",
    "    \"Then, find recent news (post-filing, from 2024) about AMD's AI chip strategy and explain \"\n",
    "    \"how this new strategy directly addresses or exacerbates one of NVIDIA's stated risks.\"\n",
    ")\n",
    "\n",
    "print(\"Executing complex query on the baseline RAG chain...\")\n",
    "\n",
    "# Invoke the chain with our challenging query\n",
    "baseline_result = baseline_rag_chain.invoke(complex_query_adv)\n",
    "\n",
    "console.print(\"\\n--- BASELINE RAG FAILED OUTPUT ---\")\n",
    "\n",
    "# Print the result using markdown formatting for readability\n",
    "console.print(Markdown(baseline_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4fb13b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Literal\n",
    "from langchain_core.documents import Document\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic model for a single step in the agent's reasoning plan\n",
    "class Step(BaseModel):\n",
    "    # A specific, answerable sub-question for this research step\n",
    "    sub_question: str = Field(\n",
    "        description=\"A specific, answerable question for this step.\"\n",
    "    )\n",
    "\n",
    "    # The agent's justification for why this step is necessary\n",
    "    justification: str = Field(\n",
    "        description=\"A brief explanation of why this step is necessary to answer the main query.\"\n",
    "    )\n",
    "\n",
    "    # The specific tool to use for this step: either internal document search or external web search\n",
    "    tool: Literal[\"search_10k\", \"search_web\"] = Field(\n",
    "        description=\"The tool to use for this step.\"\n",
    "    )\n",
    "\n",
    "    # A list of critical keywords to improve the accuracy of the search\n",
    "    keywords: List[str] = Field(\n",
    "        description=\"A list of critical keywords for searching relevant document sections.\"\n",
    "    )\n",
    "\n",
    "    # (Optional) A likely document section to perform a more targeted, filtered search within\n",
    "    document_section: Optional[str] = Field(\n",
    "        description=(\n",
    "            \"A likely document section title (e.g., 'Item 1A. Risk Factors') \"\n",
    "            \"to search within. Only for 'search_10k' tool.\"\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fee5206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic model for the overall plan, which is a list of individual steps\n",
    "class Plan(BaseModel):\n",
    "    # A list of Step objects that outlines the full research plan\n",
    "    steps: List[Step] = Field(\n",
    "        description=\"A detailed, multi-step plan to answer the user's query.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6c04f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# A TypedDict to store the results of a completed step in our research history\n",
    "class PastStep(TypedDict):\n",
    "    step_index: int  # The index of the completed step (e.g., 1, 2, 3)\n",
    "    sub_question: str  # The sub-question that was addressed in this step\n",
    "    retrieved_docs: List[Document]  # The precise documents retrieved and reranked for this step\n",
    "    summary: str  # The agent's one-sentence summary of the findings from this step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "85ae860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# The main state dictionary that will be passed between all nodes in our LangGraph agent\n",
    "class RAGState(TypedDict):\n",
    "    original_question: str  # The initial, complex query from the user that starts the process\n",
    "    plan: Plan  # The multi-step plan generated by the Planner Agent\n",
    "    past_steps: List[PastStep]  # A cumulative history of completed research steps and their findings\n",
    "    current_step_index: int  # The index of the current step in the plan being executed\n",
    "    retrieved_docs: List[Document]  # Documents retrieved in the current step (results of broad recall)\n",
    "    reranked_docs: List[Document]  # Documents after precision reranking in the current step\n",
    "    synthesized_context: str  # The concise, distilled context generated from the reranked docs\n",
    "    final_answer: str  # The final, synthesized answer to the user's original question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "172afe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from rich.pretty import pprint as rprint\n",
    "\n",
    "# The system prompt that instructs the LLM how to behave as a planner\n",
    "planner_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are an expert research planner. Your task is to create a clear, multi-step plan \n",
    "        to answer a complex user query by retrieving information from multiple sources.\n",
    "\n",
    "        You have two tools available:\n",
    "        1. `search_10k`: Use this to search for information within NVIDIA's 2023 10-K financial filing. \n",
    "           This is best for historical facts, financial data, and stated company policies or risks \n",
    "           from that specific time period.\n",
    "        2. `search_web`: Use this to search the public internet for recent news, competitor information, \n",
    "           or any topic that is not specific to NVIDIA's 2023 10-K.\n",
    "\n",
    "        Decompose the user's query into a series of simple, sequential sub-questions. \n",
    "        For each step, decide which tool is more appropriate.\n",
    "        For `search_10k` steps, also identify the most likely section of the 10-K \n",
    "        (e.g., 'Item 1A. Risk Factors', 'Item 7. Management's Discussion and Analysis...').\n",
    "        It is critical to use the exact section titles found in a 10-K filing where possible.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"User Query: {question}\"\n",
    "    )  # The user's original, complex query\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aa67ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool-Aware Planner Agent created successfully.\n",
      "\n",
      "--- Testing Planner Agent ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Plan</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">steps</span>=<span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Step</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">sub_question</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"What are NVIDIA's key risks related to competition as stated in their 2023 10-K filing?\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">justification</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Understanding NVIDIA's stated risks related to competition will provide a baseline to compare with AMD's recent strategies.\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'search_10k'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">keywords</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'competition risks'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'competitive landscape'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'market competition'</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">document_section</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Item 1A. Risk Factors'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Step</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">sub_question</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"What is AMD's AI chip strategy as of 2024?\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">justification</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Identifying AMD's current AI chip strategy will help in assessing how it might impact NVIDIA's competitive risks.\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'search_web'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">keywords</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'AMD AI chip strategy 2024'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'AMD AI products 2024'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'AMD competition NVIDIA 2024'</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">document_section</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Step</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">sub_question</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"How does AMD's 2024 AI chip strategy address or exacerbate one of NVIDIA's competition-related risks?\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">justification</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"This step will directly link AMD's strategy to NVIDIA's risks, providing a clear understanding of the competitive dynamics.\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'search_web'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">keywords</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'AMD strategy impact on NVIDIA'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'AMD AI competition NVIDIA'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'NVIDIA risks AMD 2024'</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">document_section</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPlan\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33msteps\u001b[0m=\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33msub_question\u001b[0m=\u001b[32m\"What\u001b[0m\u001b[32m are NVIDIA's key risks related to competition as stated in their 2023 10-K filing?\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mjustification\u001b[0m=\u001b[32m\"Understanding\u001b[0m\u001b[32m NVIDIA's stated risks related to competition will provide a baseline to compare with AMD's recent strategies.\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mtool\u001b[0m=\u001b[32m'search_10k'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mkeywords\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'competition risks'\u001b[0m, \u001b[32m'competitive landscape'\u001b[0m, \u001b[32m'market competition'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mdocument_section\u001b[0m=\u001b[32m'Item 1A. Risk Factors'\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33msub_question\u001b[0m=\u001b[32m\"What\u001b[0m\u001b[32m is AMD's AI chip strategy as of 2024?\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mjustification\u001b[0m=\u001b[32m\"Identifying\u001b[0m\u001b[32m AMD's current AI chip strategy will help in assessing how it might impact NVIDIA's competitive risks.\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mtool\u001b[0m=\u001b[32m'search_web'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mkeywords\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'AMD AI chip strategy 2024'\u001b[0m, \u001b[32m'AMD AI products 2024'\u001b[0m, \u001b[32m'AMD competition NVIDIA 2024'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mdocument_section\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33msub_question\u001b[0m=\u001b[32m\"How\u001b[0m\u001b[32m does AMD's 2024 AI chip strategy address or exacerbate one of NVIDIA's competition-related risks?\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mjustification\u001b[0m=\u001b[32m\"This\u001b[0m\u001b[32m step will directly link AMD's strategy to NVIDIA's risks, providing a clear understanding of the competitive dynamics.\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mtool\u001b[0m=\u001b[32m'search_web'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mkeywords\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'AMD strategy impact on NVIDIA'\u001b[0m, \u001b[32m'AMD AI competition NVIDIA'\u001b[0m, \u001b[32m'NVIDIA risks AMD 2024'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mdocument_section\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize our powerful reasoning model, as defined in the config\n",
    "reasoning_llm = ChatOpenAI(model=config[\"reasoning_llm\"], temperature=0)\n",
    "\n",
    "# Create the planner agent by piping the prompt to the LLM and instructing it to use our structured 'Plan' output\n",
    "planner_agent = planner_prompt | reasoning_llm.with_structured_output(Plan)\n",
    "print(\"Tool-Aware Planner Agent created successfully.\")\n",
    "\n",
    "# Let's test the planner agent with our complex query to see its output\n",
    "print(\"\\n--- Testing Planner Agent ---\")\n",
    "test_plan = planner_agent.invoke({\"question\": complex_query_adv})\n",
    "\n",
    "# Use rich's pretty print for a clean, readable display of the Pydantic object\n",
    "rprint(test_plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "046a39c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser  # To parse the LLM's output as a simple string\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# The prompt for our query rewriter, instructing it to act as a search expert\n",
    "query_rewriter_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are a search query optimization expert. Your task is to rewrite a given sub-question\n",
    "        into a highly effective search query for a vector database or web search engine, using keywords \n",
    "        and context from the research plan.\n",
    "\n",
    "        The rewritten query should be specific, use terminology likely to be found in the target source \n",
    "        (a financial 10-K or news articles), and be structured to retrieve the most relevant text snippets.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"Current sub-question: {sub_question}\\n\\n\"\n",
    "        \"Relevant keywords from plan: {keywords}\\n\\n\"\n",
    "        \"Context from past steps:\\n{past_context}\"\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e66d927b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Rewriter Agent created successfully.\n",
      "\n",
      "--- Testing Query Rewriter Agent ---\n",
      "Original sub-question: How does AMD's 2024 AI chip strategy potentially exacerbate the competitive risks identified in NVIDIA's 10-K?\n",
      "Rewritten Search Query: \"AMD 2024 AI chip strategy impact on NVIDIA competitive risks 10-K, MI300X vs H100, market share threat, technological change\"\n"
     ]
    }
   ],
   "source": [
    "# Create the agent by piping the prompt to our reasoning LLM and a string output parser\n",
    "query_rewriter_agent = query_rewriter_prompt | reasoning_llm | StrOutputParser()\n",
    "print(\"Query Rewriter Agent created successfully.\")\n",
    "\n",
    "# Let's test the rewriter agent. We'll pretend we've already completed the first two steps of our plan.\n",
    "print(\"\\n--- Testing Query Rewriter Agent ---\")\n",
    "\n",
    "# Test sub-question: final synthesis-style query\n",
    "test_sub_q = (\n",
    "    \"How does AMD's 2024 AI chip strategy potentially exacerbate the competitive \"\n",
    "    \"risks identified in NVIDIA's 10-K?\"\n",
    ")\n",
    "\n",
    "# Relevant keywords from the hypothetical plan\n",
    "test_keywords = [\n",
    "    \"impact\",\n",
    "    \"threaten\",\n",
    "    \"competitive pressure\",\n",
    "    \"market share\",\n",
    "    \"technological change\",\n",
    "]\n",
    "\n",
    "# Mock past context, simulating prior steps' findings\n",
    "test_past_context = (\n",
    "    \"Step 1 Summary: NVIDIA's 10-K lists intense competition and rapid technological \"\n",
    "    \"change as key risks. \"\n",
    "    \"Step 2 Summary: AMD launched its MI300X AI accelerator in 2024 to directly compete \"\n",
    "    \"with NVIDIA's H100.\"\n",
    ")\n",
    "\n",
    "# Invoke the agent with our test data\n",
    "rewritten_q = query_rewriter_agent.invoke(\n",
    "    {\n",
    "        \"sub_question\": test_sub_q,\n",
    "        \"keywords\": test_keywords,\n",
    "        \"past_context\": test_past_context,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Original sub-question: {test_sub_q}\")\n",
    "print(f\"Rewritten Search Query: {rewritten_q}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "02325903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 22 document sections.\n",
      "Section content blocks: 22\n",
      "- ITEM 1. BUSINESS\n",
      "- ITEM 1A. RISK FACTORS\n",
      "- ITEM 1B. UNRESOLVED STAFF COMMENTS\n",
      "- ITEM 2. PROPERTIES\n",
      "- ITEM 3. LEGAL PROCEEDINGS\n",
      "- ITEM 4. MINE SAFETY DISCLOSURES\n",
      "- ITEM 5. MARKET FOR REGISTRANT’S COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "- ITEM 6. [RESERVED]\n",
      "- ITEM 7. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "- ITEM 7A. QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "- ITEM 8. FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "- ITEM 9. CHANGES IN AND DISAGREEMENTS WITH ACCOUNTANTS ON ACCOUNTING AND FINANCIAL DISCLOSURE\n",
      "- ITEM 9A. CONTROLS AND PROCEDURES\n",
      "- ITEM 9B. OTHER INFORMATION\n",
      "- ITEM 9C. DISCLOSURE REGARDING FOREIGN JURISDICTIONS THAT PREVENT INSPECTIONS\n",
      "- ITEM 10. DIRECTORS, EXECUTIVE OFFICERS AND CORPORATE GOVERNANCE\n",
      "- ITEM 11. EXECUTIVE COMPENSATION\n",
      "- ITEM 12. SECURITY OWNERSHIP OF CERTAIN BENEFICIAL OWNERS AND MANAGEMENT AND RELATED STOCKHOLDER MATTERS\n",
      "- ITEM 13. CERTAIN RELATIONSHIPS AND RELATED TRANSACTIONS, AND DIRECTOR INDEPENDENCE\n",
      "- ITEM 14. PRINCIPAL ACCOUNTANT FEES AND SERVICES\n",
      "- ITEM 15. EXHIBIT AND FINANCIAL STATEMENT SCHEDULES\n",
      "- ITEM 16. FORM 10-K SUMMARY\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "raw_text = documents[0].page_content\n",
    "\n",
    "# 只匹配独立一行的 ITEM 标题：\n",
    "# - 行首可有空格\n",
    "# - ITEM + 数字(+可选字母) + '.' \n",
    "# - 后面标题内容不包含句号，一直到行尾（防止把整句正文吃进来）\n",
    "section_header_pattern = re.compile(\n",
    "    r\"\"\"\n",
    "    ^\\s*                                  # 行首空格\n",
    "    ITEM\\s+(?P<num>\\d+[A-Z]?)\\.\\s+        # ITEM 1. / ITEM 1A.\n",
    "    (?P<title>[^.\\n]{1,200}?)\\s*          # 标题：无句号，不跨行，长度限制防止乱飙\n",
    "    $                                     # 必须到行尾结束\n",
    "    \"\"\",\n",
    "    re.IGNORECASE | re.MULTILINE | re.VERBOSE\n",
    ")\n",
    "\n",
    "matches = list(section_header_pattern.finditer(raw_text))\n",
    "\n",
    "section_titles = []\n",
    "sections_content = []\n",
    "\n",
    "for i, m in enumerate(matches):\n",
    "    num = m.group(\"num\").upper()\n",
    "    title = m.group(\"title\").strip()\n",
    "\n",
    "    clean_title = f\"ITEM {num}. {title}\"\n",
    "    section_titles.append(clean_title)\n",
    "\n",
    "    start = m.end()\n",
    "    end = matches[i + 1].start() if i + 1 < len(matches) else len(raw_text)\n",
    "    content = raw_text[start:end].strip()\n",
    "    sections_content.append(content)\n",
    "\n",
    "print(f\"Identified {len(section_titles)} document sections.\")\n",
    "print(f\"Section content blocks: {len(sections_content)}\")\n",
    "for t in section_titles:\n",
    "    print(f\"- {t}\")\n",
    "\n",
    "assert len(section_titles) == len(sections_content), \"Mismatch between titles and content sections\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7e54c19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 368 chunks with section metadata.\n",
      "\n",
      "--- Sample Chunk with Metadata ---\n",
      "page_content='In evaluating NVIDIA, the following risk factors should be considered in addition to the other information in this Annual Report on Form 10-K. Purchasing or owning NVIDIA common stock involves investment risks including, but not limited to, the risks described below. Any one of the following risks could harm our business, financial condition, results of operations or reputation, which could cause our stock price to decline, and you may lose all or a part of your investment. Additional risks, trends and uncertainties not presently known to us or that we currently believe are immaterial may also harm our business, financial condition, results of operations or reputation.\n",
      "\n",
      "Risk Factors Summary\n",
      "\n",
      "Risks Related to Our Industry and Markets\n",
      "\n",
      "• Failure to meet the evolving needs of our industry and markets may adversely impact our financial results.\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "• Competition in our current and target markets could cause us to lose market share and revenue.' metadata={'section': 'ITEM 1A. RISK FACTORS', 'source_doc': 'c:\\\\Users\\\\22959\\\\Documents\\\\WXSK_Works\\\\Agentic RAG Project\\\\data\\\\nvda_10k_clean.txt', 'id': '672b09ea-2424-477a-9125-a2bd92e578c1'}\n"
     ]
    }
   ],
   "source": [
    "import uuid  # We'll use this to give each chunk a unique ID, which is good practice\n",
    "\n",
    "# This list will hold our new, metadata-rich document chunks\n",
    "doc_chunks_with_metadata = []\n",
    "\n",
    "# Loop through each section's content along with its title using enumerate\n",
    "for i, content in enumerate(sections_content):\n",
    "    # Get the corresponding title for the current content block\n",
    "    section_title = section_titles[i]\n",
    "\n",
    "    # Use the same text splitter as before, but this time, we run it ONLY on the content of the current section\n",
    "    section_chunks = text_splitter.split_text(content)\n",
    "\n",
    "    # Now, loop through the smaller chunks created from this one section\n",
    "    for chunk in section_chunks:\n",
    "        # Generate a unique ID for this specific chunk\n",
    "        chunk_id = str(uuid.uuid4())\n",
    "\n",
    "        # Create a new LangChain Document object for the chunk\n",
    "        doc_chunks_with_metadata.append(\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                # This is the most important part: we attach the metadata\n",
    "                metadata={\n",
    "                    \"section\": section_title,       # The section this chunk belongs to\n",
    "                    \"source_doc\": doc_path_clean,   # Where the document came from\n",
    "                    \"id\": chunk_id                  # The unique ID for this chunk\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"Created {len(doc_chunks_with_metadata)} chunks with section metadata.\")\n",
    "print(\"\\n--- Sample Chunk with Metadata ---\")\n",
    "\n",
    "# To prove it worked, let's find a chunk that we know should be in the 'Risk Factors' section and print it\n",
    "sample_chunk = next(\n",
    "    c\n",
    "    for c in doc_chunks_with_metadata\n",
    "    if \"risk factors\" in c.metadata.get(\"section\", \"\").lower()\n",
    ")\n",
    "print(sample_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "82d625f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class RetrievalDecision(BaseModel):\n",
    "    # The chosen retrieval strategy. Must be one of these three options.\n",
    "    strategy: Literal[\"vector_search\", \"keyword_search\", \"hybrid_search\"]\n",
    "    # The agent's justification for its choice.\n",
    "    justification: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "67304e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_supervisor_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are a retrieval strategy expert. Based on the user's query, you must decide the best retrieval strategy.\n",
    "You have three options:\n",
    "1. `vector_search`: Best for conceptual, semantic, or similarity-based queries.\n",
    "2. `keyword_search`: Best for queries with specific, exact terms, names, or codes (e.g., 'Item 1A', 'Hopper architecture').\n",
    "3. `hybrid_search`: A good default that combines both, but may be less precise than a targeted strategy.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"User Query: {sub_question}\"  # The rewritten search query will be passed here.\n",
    "    ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e008a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Supervisor Agent created.\n",
      "\n",
      "--- Testing Retrieval Supervisor Agent ---\n",
      "Query: 'revenue growth for the Compute & Networking segment in fiscal year 2023'\n",
      "Decision: keyword_search, Justification: The query is looking for specific information related to 'revenue growth', 'Compute & Networking segment', and 'fiscal year 2023'. These are precise terms and likely to be found in structured data or reports, making keyword search the most effective strategy.\n",
      "\n",
      "Query: 'general sentiment about market competition and technological innovation'\n",
      "Decision: vector_search, Justification: The query is conceptual and seeks an understanding of general sentiment, which involves interpreting the nuances of market competition and technological innovation. Vector search is best suited for capturing the semantic meaning and context of such broad topics.\n"
     ]
    }
   ],
   "source": [
    "# Create the agent by piping our prompt to the reasoning LLM and structuring its output with our Pydantic class\n",
    "retrieval_supervisor_agent = retrieval_supervisor_prompt | reasoning_llm.with_structured_output(RetrievalDecision)\n",
    "print(\"Retrieval Supervisor Agent created.\")\n",
    "\n",
    "# Let's test it with two different types of queries to see how it behaves\n",
    "print(\"\\n--- Testing Retrieval Supervisor Agent ---\")\n",
    "\n",
    "query1 = \"revenue growth for the Compute & Networking segment in fiscal year 2023\"\n",
    "decision1 = retrieval_supervisor_agent.invoke({\"sub_question\": query1})\n",
    "print(f\"Query: '{query1}'\")\n",
    "print(f\"Decision: {decision1.strategy}, Justification: {decision1.justification}\")\n",
    "\n",
    "query2 = \"general sentiment about market competition and technological innovation\"\n",
    "decision2 = retrieval_supervisor_agent.invoke({\"sub_question\": query2})\n",
    "print(f\"\\nQuery: '{query2}'\")\n",
    "print(f\"Decision: {decision2.strategy}, Justification: {decision2.justification}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "182b8e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced vector store with metadata...\n",
      "Advanced vector store created with 1477 embeddings.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # A fundamental library for numerical operations in Python\n",
    "from rank_bm25 import BM25Okapi  # The library for implementing the BM25 keyword search algorithm\n",
    "\n",
    "print(\"Creating advanced vector store with metadata...\")\n",
    "# We create a new Chroma vector store, this time using our metadata-rich chunks\n",
    "advanced_vector_store = Chroma.from_documents(\n",
    "    documents=doc_chunks_with_metadata,\n",
    "    embedding=embedding_function)\n",
    "print(f\"Advanced vector store created with {advanced_vector_store._collection.count()} embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ca848b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building BM25 index for keyword search...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBuilding BM25 index for keyword search...\")\n",
    "# Create a list where each element is a list of words from a document\n",
    "tokenized_corpus = [doc.page_content.split(\" \") for doc in doc_chunks_with_metadata]\n",
    "# Create a list of all unique document IDs\n",
    "doc_ids = [doc.metadata[\"id\"] for doc in doc_chunks_with_metadata]\n",
    "# Create a mapping from a document's ID back to the full Document object for easy lookup\n",
    "doc_map = {doc.metadata[\"id\"]: doc for doc in doc_chunks_with_metadata}\n",
    "# Initialize the BM25Okapi index with our tokenized corpus\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "80a5e629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All retrieval strategy functions ready.\n"
     ]
    }
   ],
   "source": [
    "# Strategy 1: Pure Vector Search with Metadata Filtering\n",
    "def vector_search_only(query: str, section_filter: str = None, k: int = 10):\n",
    "    # This dictionary defines the metadata filter. ChromaDB will only search documents that match this.\n",
    "    filter_dict = {\"section\": section_filter} if section_filter and \"Unknown\" not in section_filter else None\n",
    "    # Perform the similarity search with the optional filter\n",
    "    return advanced_vector_store.similarity_search(query, k=k, filter=filter_dict)\n",
    "\n",
    "# Strategy 2: Pure Keyword Search (BM25)\n",
    "def bm25_search_only(query: str, k: int = 10):\n",
    "    # Tokenize the incoming query\n",
    "    tokenized_query = query.split(\" \")\n",
    "    # Get the BM25 scores for the query against all documents in the corpus\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    # Get the indices of the top k documents\n",
    "    top_k_indices = np.argsort(bm25_scores)[::-1][:k]\n",
    "    # Use our doc_map to return the full Document objects for the top results\n",
    "    return [doc_map[doc_ids[i]] for i in top_k_indices]\n",
    "\n",
    "# Strategy 3: Hybrid Search with Reciprocal Rank Fusion (RRF)\n",
    "def hybrid_search(query: str, section_filter: str = None, k: int = 10):\n",
    "    # 1. Perform a keyword search\n",
    "    bm25_docs = bm25_search_only(query, k=k)\n",
    "    # 2. Perform a semantic search with the metadata filter\n",
    "    semantic_docs = vector_search_only(query, section_filter=section_filter, k=k)\n",
    "    # 3. Combine and re-rank the results using Reciprocal Rank Fusion (RRF)\n",
    "    # Get a unique set of all documents found by either search method\n",
    "    all_docs = {doc.metadata[\"id\"]: doc for doc in bm25_docs + semantic_docs}.values()\n",
    "    # Create lists of just the document IDs from each search result\n",
    "    ranked_lists = [[doc.metadata[\"id\"] for doc in bm25_docs], [doc.metadata[\"id\"] for doc in semantic_docs]]\n",
    "    \n",
    "    # Initialize a dictionary to store the RRF scores for each document\n",
    "    rrf_scores = {}\n",
    "    # Loop through each ranked list (BM25 and Semantic)\n",
    "    for doc_list in ranked_lists:\n",
    "        # Loop through each document ID in the list with its rank (i)\n",
    "        for i, doc_id in enumerate(doc_list):\n",
    "            if doc_id not in rrf_scores:\n",
    "                rrf_scores[doc_id] = 0\n",
    "            # The RRF formula: add 1 / (rank + k) to the score. We use k=61 as a standard default.\n",
    "            rrf_scores[doc_id] += 1 / (i + 61)\n",
    "    \n",
    "    # Sort the document IDs based on their final RRF scores in descending order\n",
    "    sorted_doc_ids = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)\n",
    "    # Return the top k Document objects based on the fused ranking\n",
    "    final_docs = [doc_map[doc_id] for doc_id in sorted_doc_ids[:k]]\n",
    "    return final_docs\n",
    "\n",
    "print(\"\\nAll retrieval strategy functions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c0901a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Keyword Search ---\n",
      "Query: Item 1A. Risk Factors\n",
      "Found 10 documents.\n",
      "- ITEM 1. BUSINESS (ID: 8c9add87-8b66-4dcd-a4e0-b6d840f015c8)\n",
      "- ITEM 1. BUSINESS (ID: 4e6a4237-df16-4170-afe9-2c067dc31e4d)\n",
      "- ITEM 1. BUSINESS (ID: d88cde03-bcdc-46e3-8d77-21cd4408b5ba)\n",
      "- ITEM 7. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS (ID: c75084c8-a1d6-4364-9e62-e6b95df4fd62)\n",
      "- ITEM 7. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS (ID: 79d09d22-bd91-47ba-b626-1221b29bed00)\n",
      "- ITEM 8. FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA (ID: 8bb72dc4-9565-41b6-9e41-7a09412b80b6)\n",
      "- ITEM 1A. RISK FACTORS (ID: 672b09ea-2424-477a-9125-a2bd92e578c1)\n",
      "- ITEM 3. LEGAL PROCEEDINGS (ID: 69a94f4d-3e86-441c-b19f-ddb6b214f139)\n",
      "- ITEM 7. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS (ID: 6c9eb122-a9e9-496a-bcdd-3818fa294ed5)\n",
      "- ITEM 5. MARKET FOR REGISTRANT’S COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES (ID: 16e57ba5-d69c-4713-afdb-c968b110db09)\n"
     ]
    }
   ],
   "source": [
    "# Test Keyword Search to see if it can precisely find a specific section\n",
    "print(\"\\n--- Testing Keyword Search ---\")\n",
    "test_query = \"Item 1A. Risk Factors\"\n",
    "test_results = bm25_search_only(test_query)\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Found {len(test_results)} documents.\")\n",
    "for res in test_results:\n",
    "    print(f\"- {res.metadata['section']} (ID: {res.metadata['id']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a0028037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CrossEncoder reranker...\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder  # The library for using cross-encoder models\n",
    "\n",
    "print(\"Initializing CrossEncoder reranker...\")\n",
    "# Initialize the CrossEncoder model using the name from our central config dictionary.\n",
    "# The library will automatically download the model from the Hugging Face Hub if it's not cached.\n",
    "reranker = CrossEncoder(config[\"reranker_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "802f9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_documents_function(query: str, documents: List[Document]) -> List[Document]:\n",
    "    # If we have no documents to rerank, return an empty list immediately.\n",
    "    if not documents:\n",
    "        return []\n",
    "\n",
    "    # Create the pairs of [query, document_content] that the cross-encoder needs.\n",
    "    pairs = [(query, doc.page_content) for doc in documents]\n",
    "\n",
    "    # Use the reranker to predict a relevance score for each pair. This returns a list of scores.\n",
    "    scores = reranker.predict(pairs)\n",
    "\n",
    "    # Combine the original documents with their new scores.\n",
    "    doc_scores = list(zip(documents, scores))\n",
    "\n",
    "    # Sort the list of (document, score) tuples in descending order based on the score.\n",
    "    doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extract just the Document objects from the top N sorted results.\n",
    "    # The number of documents to keep is controlled by 'top_n_rerank' in our config.\n",
    "    reranked_docs = [doc for doc, score in doc_scores[:config[\"top_n_rerank\"]]]\n",
    "\n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "de8e0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompt for our distiller agent, instructing it to synthesize and be concise\n",
    "distiller_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a helpful assistant. Your task is to synthesize the \n",
    "            following retrieved document snippets into a single, concise paragraph.\n",
    "            The goal is to provide a clear and coherent context that directly \n",
    "            answers the question: '{question}'. Focus on removing redundant \n",
    "            information and organizing the content logically. \n",
    "            Answer only with the synthesized context.\"\"\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\", \n",
    "            \"Retrieved Documents:\\n{context}\"\n",
    "        )  # The content of our top 3 reranked documents will be passed here\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "21df05f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Distiller Agent created.\n"
     ]
    }
   ],
   "source": [
    "# Create the agent by piping our prompt to the reasoning LLM and a string output parser\n",
    "distiller_agent = distiller_prompt | reasoning_llm | StrOutputParser()\n",
    "print(\"Contextual Distiller Agent created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a667ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "# Initialize the Tavily search tool.\n",
    "# k=3: This parameter instructs the tool to return the top 3 most relevant search results for a given query.\n",
    "web_search_tool = TavilySearch(max_results=3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "469e02fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search_function(query: str) -> List[Document]:\n",
    "    # 调用 Tavily 工具\n",
    "    resp: dict = web_search_tool.invoke({\"query\": query})\n",
    "    # resp 是一个 dict，我们从中取出 \"results\" 列表\n",
    "    results = resp.get(\"results\", [])\n",
    "\n",
    "    docs: List[Document] = []\n",
    "    for item in results:\n",
    "        item: dict\n",
    "        # Tavily 返回里通常有 content / url / title 等字段\n",
    "        content = item.get(\"content\") or item.get(\"raw_content\") or \"\"\n",
    "        if not content:\n",
    "            continue\n",
    "        url = item.get(\"url\", \"\")\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=content,\n",
    "                metadata={\"source\": url} if url else {}\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f7207b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Web Search Tool ---\n",
      "Found 5 results for query: 'AMD AI chip strategy 2024'\n",
      "Top result snippet: Feb 3(Reuters) - AMD (AMD.O), opens new tab investors will closely examine the chip designer's artificial intelligence strategy when it reports fourth-quarter results on Tuesday as Big Tech's shift to custom silicon raises doubts about its place in t...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Testing Web Search Tool ---\")\n",
    "test_query_web = \"AMD AI chip strategy 2024\"\n",
    "test_results_web = web_search_function(test_query_web)\n",
    "print(f\"Found {len(test_results_web)} results for query: '{test_query_web}'\")\n",
    "if test_results_web:\n",
    "    print(f\"Top result snippet: {test_results_web[0].page_content[:250]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "12bbf829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompt for our reflection agent, instructing it to be concise and factual\n",
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a research assistant. Based on the retrieved context \n",
    "            for the current sub-question, write a concise, one-sentence summary \n",
    "            of the key findings. This summary will be added to our research history. \n",
    "            Be factual and to the point.\"\"\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Current sub-question: {sub_question}\\n\\nDistilled context:\\n{context}\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "89e84445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reflection Agent created.\n"
     ]
    }
   ],
   "source": [
    "# Create the agent by piping our prompt to the reasoning LLM and a string output parser\n",
    "reflection_agent = reflection_prompt | reasoning_llm | StrOutputParser()\n",
    "print(\"Reflection Agent created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "79777c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision(BaseModel):\n",
    "    # The decision must be one of these two actions.\n",
    "    next_action: Literal[\"CONTINUE_PLAN\", \"FINISH\"]\n",
    "    # The agent must justify its decision.\n",
    "    justification: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500494d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompt for our policy agent, instructing it to act as a master strategist\n",
    "policy_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a master strategist. Your role is to analyze the research progress \n",
    "            and decide the next action. You have the original question, the initial plan, \n",
    "            and a log of completed steps with their summaries.\n",
    "            - If the collected information in the Research History is sufficient to \n",
    "            comprehensively answer the Original Question, decide to FINISH.\n",
    "            - Otherwise, if the plan is not yet complete, decide to CONTINUE_PLAN.\"\"\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Original Question: {question}\\n\\nInitial Plan:\\n{plan}\\n\\nResearch History (Completed Steps):\\n{history}\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "815abd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Agent created.\n",
      "\n",
      "--- Testing Policy Agent (Incomplete State) ---\n",
      "Decision: CONTINUE_PLAN, Justification: The research so far has only identified the general competitive nature of the semiconductor industry as a risk for NVIDIA. However, the specific details of NVIDIA's key risks related to competition, as outlined in their 2023 10-K filing, have not been fully explored. Additionally, we have not yet gathered any information on AMD's 2024 AI chip strategy, which is crucial to understanding how it might impact NVIDIA's competitive position. Therefore, it is necessary to continue with the plan to gather comprehensive information on both NVIDIA's specific risks and AMD's strategy.\n",
      "\n",
      "--- Testing Policy Agent (Complete State) ---\n",
      "Decision: CONTINUE_PLAN, Justification: The research has identified NVIDIA's general competitive risks and AMD's recent AI chip strategy. However, the analysis of how AMD's strategy specifically addresses or exacerbates NVIDIA's stated risks is still pending. Completing this step is crucial to directly link AMD's actions to NVIDIA's competitive risks, thereby fully answering the original question.\n"
     ]
    }
   ],
   "source": [
    "# Create the agent by piping our prompt to the reasoning LLM \n",
    "# and structuring its output with our Decision class\n",
    "policy_agent = (\n",
    "    policy_prompt \n",
    "    | reasoning_llm.with_structured_output(Decision)\n",
    ")\n",
    "print(\"Policy Agent created.\")\n",
    "\n",
    "# Now, let's test the policy agent with two different states of our research process\n",
    "print(\"\\n--- Testing Policy Agent (Incomplete State) ---\")\n",
    "\n",
    "# First, a state where only Step 1 is complete.\n",
    "plan_str = json.dumps(\n",
    "    [s.model_dump() for s in test_plan.steps]\n",
    ")\n",
    "incomplete_history = (\n",
    "    \"Step 1 Summary: NVIDIA's 10-K states that the semiconductor industry \"\n",
    "    \"is intensely competitive and subject to rapid technological change.\"\n",
    ")\n",
    "\n",
    "decision1 = policy_agent.invoke(\n",
    "    {\n",
    "        \"question\": complex_query_adv,\n",
    "        \"plan\": plan_str, \n",
    "        \"history\": incomplete_history\n",
    "    }\n",
    ")\n",
    "print(\n",
    "    f\"Decision: {decision1.next_action}, \"\n",
    "    f\"Justification: {decision1.justification}\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- Testing Policy Agent (Complete State) ---\")\n",
    "\n",
    "# Second, a state where both Step 1 and Step 2 are complete.\n",
    "complete_history = (\n",
    "    incomplete_history \n",
    "    + \"\\nStep 2 Summary: In 2024, AMD launched its MI300X accelerator \"\n",
    "    \"to directly compete with NVIDIA in the AI chip market, \"\n",
    "    \"gaining adoption from major cloud providers.\"\n",
    ")\n",
    "\n",
    "decision2 = policy_agent.invoke(\n",
    "    {\n",
    "        \"question\": complex_query_adv,\n",
    "        \"plan\": plan_str,\n",
    "        \"history\": complete_history\n",
    "    }\n",
    ")\n",
    "print(\n",
    "    f\"Decision: {decision2.next_action}, \"\n",
    "    f\"Justification: {decision2.justification}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e3fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
