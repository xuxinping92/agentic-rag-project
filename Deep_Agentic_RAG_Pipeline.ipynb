{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7939d86",
   "metadata": {},
   "source": [
    "## 一、配置环境\n",
    "\n",
    "在开始编写深度 RAG Pipeline 代码之前，我们需要配置环境。当我们开始开发管道并对其进行反复试验时，最好以简单的字典格式定义我们的配置，因为稍后当管道变得复杂时，我们可以简单地参考该字典来更改配置并查看其对整体性能的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40285711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Configuration Dictionary to manage all system parameters\n",
    "config = {\n",
    "    \"data_dir\": \"./data\", # Directory to store raw and cleaned data\n",
    "    \"vector_store_dir\": \"./vector_store\", # Directory to persist our vector store\n",
    "    \"llm_provider\": \"openai\", # The LLM provider we are using\n",
    "    \"reasoning_llm\":\"gpt-4o\", # The powerful model for planning and synthesis\n",
    "    \"fast_llm\": \"gpt-4o-mini\", # A faster, cheaper model for simpler tasks like the baseline RAG\n",
    "    \"embedding_model\": \"text-embedding-3-small\", # The model for creating document embeddings\n",
    "    \"reranker_model\":\"cross-encoder/ms-marco-MiniLM-L-6-v2\", # The model for precision reranking\n",
    "    \"max_reasoning_iterations\": 7, # A safeguard to prevent the agent from getting into an infinite loop\n",
    "    \"top_k_retrieval\": 10, # Number of documents for initial broad recall\n",
    "    \"top_n_rerank\": 3, # Number of documents to keep after precision reranking\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37fcf67",
   "metadata": {},
   "source": [
    "这些配置参数很容易理解，但有三个配置参数值得一提：\n",
    "\n",
    "- **`llm_provider`**：这是我们使用的 LLM 提供商，在本例中是 OpenAI。我使用 OpenAI 是因为我们可以轻松地在 LangChain 中切换模型和提供商，但您也可以选择任何适合您需求的提供商，例如 Ollama。\n",
    "\n",
    "- **`reasoning_llm`**：这必须是我们整个设置中最强大的，因为它将用于规划和综合。\n",
    "\n",
    "- **`fast_llm`**：这应该是一个更快、更便宜的模型，因为它将用于更简单的任务，如 baseline RAG。\n",
    "\n",
    "现在需要导入管道使用的所需库，并将 api 密钥设置为环境变量，避免在代码块中暴露它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c93c5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # For interacting with the operating system (e.g., managing environment variables)\n",
    "import re # For regular expression operations, useful for text cleaning\n",
    "import json # For working with JSON data\n",
    "from getpass import getpass # To securely prompt for user input like API keys without echoing to the screen\n",
    "from pprint import pprint # For pretty-printing Python objects, making them more readable\n",
    "import uuid # To generate unique identifiers\n",
    "from typing import List, Dict, TypedDict, Literal, Optional # For type hinting to create clean, readable, and maintainable code\n",
    "\n",
    "# Helper function to securely set environment variables if they are not already present\n",
    "def _set_env(var: str):\n",
    "    # Check if the environment variable is not already set\n",
    "    if not os.environ.get(var): # If not, prompt the user to enter it securely\n",
    "        os.environ[var] = getpass(f\"Enter your{var}: \")\n",
    "\n",
    "# Set the API keys for the services we will use\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# For accessing OpenAI models (GPT-4o, embeddings)\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "# For tracing and debugging with LangSmith\n",
    "_set_env(\"TAVILY_API_KEY\") # For the web search tool\n",
    "# Enable LangSmith tracing to get detailed logs and visualizations of our agent's execution\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# Define a project name in LangSmith to organize our runs\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"Advanced-Deep-Thinking-RAG\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f6760",
   "metadata": {},
   "source": [
    "这里还启用了 LangSmith 的追踪功能。当您使用复杂、循环工作流的代理系统时，它的追踪功能可以帮助您直观地了解正在发生的事情，并使调试代理的思维过程变得更加容易。\n",
    "\n",
    "## 二、获取知识库\n",
    "\n",
    "生产级 RAG 系统需要复杂且严苛的知识库才能真正展现其有效性。为此，我们将使用 NVIDIA 2023 年的 10-K 文件（\n",
    "https://www.sec.gov/Archives/edgar/data/1045810/000104581023000017/nvda-20230129.htm\n",
    "），这是一份超过一百页的综合性文件，详细说明了公司的业务运营、财务业绩和披露的风险因素。\n",
    "\n",
    "首先，我们自定义一个函数直接从 SEC EDGAR 数据库下载 10-K 文件，解析原始 HTML，并将其转换为适合我们的 RAG 管道提取的干净结构化文本格式。函数代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_local_10k(html_path, clean_path):\n",
    "    \"\"\"\n",
    "    Parse a locally downloaded SEC 10-K iXBRL HTML file and extract cleaned human-readable text.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Parsing local 10-K file: {html_path}\")\n",
    "\n",
    "    with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # 1) 去掉明显不需要的标签\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # 2) 去掉带命名空间的标签 (iXBRL: ix:, xbrli:, us-gaap: 等)\n",
    "    #    BeautifulSoup 会把它们当成名字里带冒号的 tag\n",
    "    for tag in soup.find_all():\n",
    "        if \":\" in tag.name:\n",
    "            tag.decompose()\n",
    "\n",
    "    # 3) 从常见容器中抽取文本\n",
    "    text_parts = []\n",
    "    for tag in soup.find_all([\"p\", \"div\", \"li\", \"td\"]):\n",
    "        t = tag.get_text(\" \", strip=True)\n",
    "        if not t:\n",
    "            continue\n",
    "\n",
    "        # 过滤明显是 XBRL / 垃圾行\n",
    "        if any(bad in t for bad in [\"us-gaap:\", \"xbrli:\", \"iso4217:\", \"dei:\"]):\n",
    "            continue\n",
    "        if len(t) < 5:\n",
    "            continue\n",
    "\n",
    "        text_parts.append(t)\n",
    "\n",
    "    # 4) 合并 & 清洗\n",
    "    text = \"\\n\\n\".join(text_parts)\n",
    "\n",
    "    # 连续空行压缩\n",
    "    clean_text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    # 多空格压缩\n",
    "    clean_text = re.sub(r\"[ \\t]{2,}\", \" \", clean_text).strip()\n",
    "\n",
    "    os.makedirs(os.path.dirname(clean_path), exist_ok=True)\n",
    "    with open(clean_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(clean_text)\n",
    "\n",
    "    print(f\"✅ Cleaned text saved to: {clean_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1415d6fb",
   "metadata": {},
   "source": [
    "代码很容易理解，使用 beautifulsoup4 来解析 HTML 内容并提取文本。它将帮助我们轻松浏览 HTML 结构并检索相关信息，同时忽略任何不必要的元素，例如脚本或样式。\n",
    "\n",
    "现在，让我们执行它并看看它是如何工作的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c7e8574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录： c:\\Users\\22959\\Documents\\WXSK_Works\\Agentic RAG Project\n",
      "原始文件路径： c:\\Users\\22959\\Documents\\WXSK_Works\\Agentic RAG Project\\data\\nvda_10k_raw.html\n",
      "清洗文件路径： c:\\Users\\22959\\Documents\\WXSK_Works\\Agentic RAG Project\\data\\nvda_10k_clean.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 定义数据文件夹\n",
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "os.makedirs(data_dir, exist_ok=True)  # 如果不存在则自动创建\n",
    "\n",
    "# 三个变量定义\n",
    "url_10k = \"https://www.sec.gov/Archives/edgar/data/1045810/000104581023000017/nvda-20230129.htm\"\n",
    "doc_path_raw = os.path.join(data_dir, \"nvda_10k_raw.html\")\n",
    "doc_path_clean = os.path.join(data_dir, \"nvda_10k_clean.txt\")\n",
    "\n",
    "print(\"当前工作目录：\", os.getcwd())\n",
    "print(\"原始文件路径：\", doc_path_raw)\n",
    "print(\"清洗文件路径：\", doc_path_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54cf601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing local 10-K file: c:\\Users\\22959\\Documents\\WXSK_Works\\Agentic RAG Project\\data\\nvda_20230129.htm\n",
      "✅ Cleaned text saved to: c:\\Users\\22959\\Documents\\WXSK_Works\\Agentic RAG Project\\data\\nvda_10k_clean.txt\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "html_path = os.path.join(data_dir, \"nvda_20230129.htm\")\n",
    "clean_path = os.path.join(data_dir, \"nvda_10k_clean.txt\")\n",
    "\n",
    "parse_local_10k(html_path, clean_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb73cf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preview ---\n",
      "\n",
      "PART I\n",
      "\n",
      "ITEM 1. BUSINESS\n",
      "\n",
      "Our Company\n",
      "\n",
      "NVIDIA pioneered accelerated computing to help solve the most challenging computational problems. Since our original focus on PC graphics, we have expanded to several other large and important computationally intensive fields. Fueled by the sustained demand for exceptional 3D graphics and the scale of the gaming market, NVIDIA has leveraged its GPU architecture to create platforms for scientific computing, artificial intelligence, or AI, data science, autonomous vehicles, or AV, robotics, metaverse and 3D internet applications.\n",
      "\n",
      "The GPU was initially used to simulate human imagination, enabling the virtual worlds of video games and films. Today, it also simulates human intelligence, enabling a deeper understanding of the physical world. Its parallel processing capabilities, supported by thousands of computing cores, are essential to running deep learning algorithms. This form of AI, in which software writes itself by learning from large amounts of...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Preview ---\\n\")\n",
    "with open(clean_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    clean_text = f.read()\n",
    "    print(clean_text[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ff7f8",
   "metadata": {},
   "source": [
    "我们只是调用这个函数将所有内容存储在一个 txt 文件中，该文件将作为我们的 RAG 管道的上下文。\n",
    "\n",
    "当我们运行上述代码时，您可以看到它开始为我们下载报告，并且我们可以看到下载内容的样本是什么样的。\n",
    "\n",
    "## 三、理解我们的多源、多跳查询\n",
    "\n",
    "为了测试我们实施的管道并将其与基本 RAG 进行比较，我们需要使用一个非常复杂的查询，涵盖我们知识库文档的不同方面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "632b4334",
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_query = (\n",
    "    \"Based on NVIDIA's 2023 10-K filing, identify their key risks related to competition. \"\n",
    "    \"Then, find recent news (post-filing, from 2024) about AMD's AI chip strategy and explain \"\n",
    "    \"how this new strategy directly addresses or exacerbates one of NVIDIA's stated risks.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9116b15",
   "metadata": {},
   "source": [
    "让我们分析一下为什么这个查询对于标准 RAG 管道来说如此困难：\n",
    "\n",
    "- **多跳推理**：系统必须首先识别风险，然后找到 AMD 消息，最后将两者综合起来。\n",
    "- **多源知识**：所需信息存在于两个完全不同的地方。风险信息存在于我们静态的内部文档（10-K）中，而 AMD 新闻则位于外部，需要访问实时网络。\n",
    "- **综合与分析**：该查询并非要求简单地列出事实。它要求解释一组事实如何使另一组事实恶化，这项任务需要真正的综合能力。\n",
    "\n",
    "在下一节中，我们将实现基本的 RAG 管道，实际上看看简单的 RAG 是如何失败的。\n",
    "\n",
    "## 四、构建一个将会失败的浅层 RAG 管道\n",
    "\n",
    "现在我们已经配置好了环境，并且准备好了具有挑战性的知识库，下一步就是构建一个标准的原生 RAG 流水线。\n",
    "\n",
    "首先构建最简单的解决方案，我们可以对其运行复杂的查询，并观察它失败的原因和方式。\n",
    "\n",
    "- **加载和分块文档**：我们将提取已清理的 10-K 文件并将其拆分成小的、固定大小的块，这是一种常见但语义上简单的方法。\n",
    "- **创建向量存储**：然后我们将嵌入这些块并将它们索引到 ChromaDB 向量存储中，以实现基本的语义搜索。\n",
    "- **组装 RAG 链**：我们将使用 LangChain 表达语言 (LCEL) 将我们的检索器、提示模板和 LLM 连接成一个线性管道。\n",
    "- **演示复杂查询**：我们将针对这个简单的系统执行多跳、多源查询，并分析其不充分的响应。\n",
    "\n",
    "首先，我们需要加载已清理的文档并将其拆分。我们将使用 LangChain 生态系统中的标准工具 RecursiveCharacterTextSplitter。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "70564000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and chunking the document...\n",
      "Document loaded and split into 359 chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader  # 文本加载器\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # 文本切分器\n",
    "\n",
    "\n",
    "print(\"Loading and chunking the document...\")\n",
    "\n",
    "# Initialize the loader with the path to our cleaned 10-K file\n",
    "loader = TextLoader(doc_path_clean, encoding='utf-8')\n",
    "\n",
    "# Load the document into memory\n",
    "documents = loader.load()\n",
    "\n",
    "# Initialize the text splitter with a defined chunk size and overlap\n",
    "# chunk_size=1000: Each chunk will be approximately 1000 characters long.\n",
    "# chunk_overlap=150: Each chunk will share 150 characters with the previous one to maintain context.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150\n",
    ")\n",
    "\n",
    "# Split the loaded document into smaller, manageable chunks\n",
    "doc_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Document loaded and split into {len(doc_chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12230f45",
   "metadata": {},
   "source": [
    "我们的主文档中有 378 个数据块，下一步创建向量嵌入并将其存储在数据库中。我们将使用 ChromaDB（一种流行的内存向量存储）和配置中定义的 text-embedding-3-small 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "96f302c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating baseline vector store...\n",
      "Vector store created with 1086 embeddings.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma  # The vector store we will use\n",
    "from langchain_openai import OpenAIEmbeddings  # The function to create embeddings\n",
    "\n",
    "print(\"Creating baseline vector store...\")\n",
    "\n",
    "# Initialize the embedding function using the model specified in our config\n",
    "embedding_function = OpenAIEmbeddings(model=config['embedding_model'])\n",
    "\n",
    "# Create the Chroma vector store from our document chunks\n",
    "# This process takes each chunk, creates an embedding for it, and indexes it.\n",
    "baseline_vector_store = Chroma.from_documents(\n",
    "    documents=doc_chunks,\n",
    "    embedding=embedding_function\n",
    ")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "# The retriever is the component that will actually perform the search.\n",
    "# search_kwargs={\"k\": 3}: This tells the retriever to return the top 3 most relevant chunks for any given query.\n",
    "baseline_retriever = baseline_vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(f\"Vector store created with {baseline_vector_store._collection.count()} embeddings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904c2006",
   "metadata": {},
   "source": [
    "Chroma.from_documents 将所有向量存储在一个可搜索的索引中。最后一步是使用 LangChain 表达式语言 (LCEL) 将它们组装成一个可运行的 RAG 链。\n",
    "\n",
    "该链将定义数据的线性流：从用户的问题到检索器，再到提示，最后到 LLM。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4daca5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate  # For creating prompt templates\n",
    "from langchain_openai import ChatOpenAI  # The OpenAI chat model interface\n",
    "from langchain_core.runnables import RunnablePassthrough  # A tool to pass inputs through the chain\n",
    "from langchain_core.output_parsers import StrOutputParser  # To parse the LLM's output as a simple string\n",
    "\n",
    "# This template instructs the LLM on how to behave.\n",
    "# {context}: This is where we will inject the content from our retrieved documents.\n",
    "# {question}: This is where the user's original question will go.\n",
    "template = \"\"\"\n",
    "You are an AI financial analyst. Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt template from the text\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# We use our 'fast_llm' for this simple task, as defined in our config\n",
    "llm = ChatOpenAI(model=config[\"fast_llm\"], temperature=0)\n",
    "\n",
    "# A helper function to format the list of retrieved documents into a single string\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n---\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# The complete RAG chain defined using LCEL's pipe (|) syntax\n",
    "baseline_rag_chain = (\n",
    "    {\n",
    "        \"context\": baseline_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    # The context is generated by taking the question, passing it to the retriever, and formatting the result\n",
    "    # The original question is passed through unchanged\n",
    "    | prompt  # The dictionary is then passed to the prompt template\n",
    "    | llm  # The formatted prompt is passed to the language model\n",
    "    | StrOutputParser()  # The LLM's output message is parsed into a string\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0248d667",
   "metadata": {},
   "source": [
    "首先，定义了提示词模板，它的 `context` 键由子链填充，输入问题来自 `baseline_retriever`，其输出（Document 对象列表）是通过 `format_docs` 函数格式化为单个字符串。`question` 键只需使用 `RunnablePassthrough` 传递原始输入即可。\n",
    "\n",
    "让我们运行这个简单的管道并了解它在哪里失败了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2b907d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing complex query on the baseline RAG chain...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--- BASELINE RAG FAILED OUTPUT ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--- BASELINE RAG FAILED OUTPUT ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Based on NVIDIA's 2023 10-K filing, the key risks related to competition include:                                  \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">Competition in Current and Target Markets</span>: NVIDIA faces competition in its existing markets as well as in new   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>markets it aims to enter. This competition could lead to a loss of market share and revenue.                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">Failure to Meet Evolving Industry Needs</span>: If NVIDIA fails to adapt to the changing demands of the industry and   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>its markets, it may adversely affect its financial results.                                                     \n",
       "\n",
       "Regarding AMD's AI chip strategy, while I cannot access real-time news or updates post-2023, I can provide a       \n",
       "hypothetical analysis based on typical industry trends. If AMD has recently announced a new AI chip strategy that  \n",
       "focuses on enhancing performance and efficiency in AI applications, this could directly address NVIDIA's stated    \n",
       "risk of competition.                                                                                               \n",
       "\n",
       "For instance, if AMD's new AI chips are designed to offer superior performance at a lower cost or with better      \n",
       "energy efficiency compared to NVIDIA's offerings, this could exacerbate NVIDIA's competitive risk by attracting    \n",
       "customers away from NVIDIA. Such advancements could lead to increased market share for AMD, thereby impacting      \n",
       "NVIDIA's revenue and market position negatively. Additionally, if AMD's strategy includes aggressive pricing or    \n",
       "partnerships that enhance its market presence, it could further challenge NVIDIA's ability to meet the evolving    \n",
       "needs of the industry, as customers may gravitate towards AMD's solutions.                                         \n",
       "\n",
       "In summary, AMD's AI chip strategy could directly address NVIDIA's competitive risks by providing alternative,     \n",
       "potentially superior options for customers in the AI market, thereby threatening NVIDIA's market share and         \n",
       "financial performance.                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Based on NVIDIA's 2023 10-K filing, the key risks related to competition include:                                  \n",
       "\n",
       "\u001b[1;33m 1 \u001b[0m\u001b[1mCompetition in Current and Target Markets\u001b[0m: NVIDIA faces competition in its existing markets as well as in new   \n",
       "\u001b[1;33m   \u001b[0mmarkets it aims to enter. This competition could lead to a loss of market share and revenue.                    \n",
       "\u001b[1;33m 2 \u001b[0m\u001b[1mFailure to Meet Evolving Industry Needs\u001b[0m: If NVIDIA fails to adapt to the changing demands of the industry and   \n",
       "\u001b[1;33m   \u001b[0mits markets, it may adversely affect its financial results.                                                     \n",
       "\n",
       "Regarding AMD's AI chip strategy, while I cannot access real-time news or updates post-2023, I can provide a       \n",
       "hypothetical analysis based on typical industry trends. If AMD has recently announced a new AI chip strategy that  \n",
       "focuses on enhancing performance and efficiency in AI applications, this could directly address NVIDIA's stated    \n",
       "risk of competition.                                                                                               \n",
       "\n",
       "For instance, if AMD's new AI chips are designed to offer superior performance at a lower cost or with better      \n",
       "energy efficiency compared to NVIDIA's offerings, this could exacerbate NVIDIA's competitive risk by attracting    \n",
       "customers away from NVIDIA. Such advancements could lead to increased market share for AMD, thereby impacting      \n",
       "NVIDIA's revenue and market position negatively. Additionally, if AMD's strategy includes aggressive pricing or    \n",
       "partnerships that enhance its market presence, it could further challenge NVIDIA's ability to meet the evolving    \n",
       "needs of the industry, as customers may gravitate towards AMD's solutions.                                         \n",
       "\n",
       "In summary, AMD's AI chip strategy could directly address NVIDIA's competitive risks by providing alternative,     \n",
       "potentially superior options for customers in the AI market, thereby threatening NVIDIA's market share and         \n",
       "financial performance.                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console  # For pretty-printing output with markdown\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# Initialize the rich console for better output formatting\n",
    "console = Console()\n",
    "\n",
    "# Our complex, multi-hop, multi-source query\n",
    "complex_query_adv = (\n",
    "    \"Based on NVIDIA's 2023 10-K filing, identify their key risks related to competition. \"\n",
    "    \"Then, find recent news (post-filing, from 2024) about AMD's AI chip strategy and explain \"\n",
    "    \"how this new strategy directly addresses or exacerbates one of NVIDIA's stated risks.\"\n",
    ")\n",
    "\n",
    "print(\"Executing complex query on the baseline RAG chain...\")\n",
    "\n",
    "# Invoke the chain with our challenging query\n",
    "baseline_result = baseline_rag_chain.invoke(complex_query_adv)\n",
    "\n",
    "console.print(\"\\n--- BASELINE RAG FAILED OUTPUT ---\")\n",
    "\n",
    "# Print the result using markdown formatting for readability\n",
    "console.print(Markdown(baseline_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353028e4",
   "metadata": {},
   "source": [
    "分析一下该 RAG 管道失败的三个方面：\n",
    "\n",
    "- **不相关的上下文**：Retriever 抓取了 \"NVIDIA\"、\"competition\" 和 \"AMD\" 的一般信息，但没有抓取到 2024 年 AMD 战略的具体细节。\n",
    "- **信息缺失**：关键缺陷在于 2023 年的数据无法涵盖 2024 年的事件。系统没有意识到自己缺少关键信息。\n",
    "- **缺乏规划或工具使用**：将复杂的查询视为简单的查询，无法将其分解为多个步骤，也无法使用网页搜索等工具来填补空白。\n",
    "\n",
    "该系统的失败并非因为 LLM 太笨，而是因为其架构过于简单。它试图用一个线性的、一次性的过程来解决一个循环的、多步骤的问题。\n",
    "\n",
    "我们已经了解了基本 RAG 管道的问题，接下来让我们看看深度思考 RAG 是如何解决好复杂查询的。\n",
    "\n",
    "## 五、为 Agent 系统定义 RAGState\n",
    "\n",
    "要构建推理 agent，我们首先需要一种方法来管理它的状态。在我们简单的 RAG 链中，每个步骤都是无状态的，但是智能 agent 需要记忆。它需要记住最初的问题、制定的计划以及迄今为止收集的证据。\n",
    "\n",
    "RAGState 将充当中央内存，在 LangGraph 工作流程中的每个节点之间传递。为了构建它，我们将定义一系列结构化数据类，从最基本的构建块开始：研究计划中的单个步骤。\n",
    "\n",
    "我们希望定义智能体计划的原子单元。每个 Step 不仅要包含一个需要回答的问题，还要包含其背后的推理，以及至关重要的是，智能体应该使用的具体工具。这迫使智能体的规划过程变得清晰明确且结构化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fb13b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Literal\n",
    "from langchain_core.documents import Document\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic model for a single step in the agent's reasoning plan\n",
    "class Step(BaseModel):\n",
    "    # A specific, answerable sub-question for this research step\n",
    "    sub_question: str = Field(\n",
    "        description=\"A specific, answerable question for this step.\"\n",
    "    )\n",
    "\n",
    "    # The agent's justification for why this step is necessary\n",
    "    justification: str = Field(\n",
    "        description=\"A brief explanation of why this step is necessary to answer the main query.\"\n",
    "    )\n",
    "\n",
    "    # The specific tool to use for this step: either internal document search or external web search\n",
    "    tool: Literal[\"search_10k\", \"search_web\"] = Field(\n",
    "        description=\"The tool to use for this step.\"\n",
    "    )\n",
    "\n",
    "    # A list of critical keywords to improve the accuracy of the search\n",
    "    keywords: List[str] = Field(\n",
    "        description=\"A list of critical keywords for searching relevant document sections.\"\n",
    "    )\n",
    "\n",
    "    # (Optional) A likely document section to perform a more targeted, filtered search within\n",
    "    document_section: Optional[str] = Field(\n",
    "        description=(\n",
    "            \"A likely document section title (e.g., 'Item 1A. Risk Factors') \"\n",
    "            \"to search within. Only for 'search_10k' tool.\"\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc231255",
   "metadata": {},
   "source": [
    "Step 类使用了 Pydantic BaseModel，定义了 Planner Agent 严格执行的规范。`tool: Literal[...]` 字段定义 LLM 需要做出具体的决策，使用我们的内部知识（`search_10k`）还是寻求外部信息（`search_web`）。这种结构化的输出比尝试解析自然语言计划要可靠得多。\n",
    "\n",
    "现在我们已经定义了一个 Step，我们需要一个容器来保存整个步骤序列。我们将创建一个 Plan 类，它只是一个 Step 对象的列表。这代表了代理完整的端到端研究策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fee5206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic model for the overall plan, which is a list of individual steps\n",
    "class Plan(BaseModel):\n",
    "    # A list of Step objects that outlines the full research plan\n",
    "    steps: List[Step] = Field(\n",
    "        description=\"A detailed, multi-step plan to answer the user's query.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8089021e",
   "metadata": {},
   "source": [
    "我们编写了一个 Plan 类，它将为整个研究过程提供结构。当我们调用 Planner Agent 时，我们会要求它返回一个符合此模式的 JSON 对象。这确保了在执行任何检索操作之前，代理策略清晰、有序且机器可读。\n",
    "\n",
    "接下来，当我们的智能体执行其计划时，它需要一种方法来记住它所学到的知识。我们将定义一个 PastStep 字典来存储每个已完成步骤的结果。这将构成智能体的研究历史或实验室笔记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c04f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# A TypedDict to store the results of a completed step in our research history\n",
    "class PastStep(TypedDict):\n",
    "    step_index: int  # The index of the completed step (e.g., 1, 2, 3)\n",
    "    sub_question: str  # The sub-question that was addressed in this step\n",
    "    retrieved_docs: List[Document]  # The precise documents retrieved and reranked for this step\n",
    "    summary: str  # The agent's one-sentence summary of the findings from this step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db268f3d",
   "metadata": {},
   "source": [
    "PastStep 结构对于智能体的自我批评循环至关重要。每完成一步，我们都会填充其中一个字典并将其添加到状态中。然后，智能体将能够查看这个不断增长的摘要列表，以了解其已知信息，并确定是否拥有足够的信息来完成其任务。\n",
    "\n",
    "最后，我们将所有这些部分整合到主 RAGState 字典中。它是贯穿整个图的核心对象，包含原始查询、完整计划、过去步骤的历史记录以及当前执行步骤的所有中间数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85ae860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# The main state dictionary that will be passed between all nodes in our LangGraph agent\n",
    "class RAGState(TypedDict):\n",
    "    original_question: str  # The initial, complex query from the user that starts the process\n",
    "    plan: Plan  # The multi-step plan generated by the Planner Agent\n",
    "    past_steps: List[PastStep]  # A cumulative history of completed research steps and their findings\n",
    "    current_step_index: int  # The index of the current step in the plan being executed\n",
    "    retrieved_docs: List[Document]  # Documents retrieved in the current step (results of broad recall)\n",
    "    reranked_docs: List[Document]  # Documents after precision reranking in the current step\n",
    "    synthesized_context: str  # The concise, distilled context generated from the reranked docs\n",
    "    final_answer: str  # The final, synthesized answer to the user's original question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca4848",
   "metadata": {},
   "source": [
    "这个 RAGState(TypedDict) 是我们 agent 的完整大脑。图中的每个节点都会接收这个字典作为输入，并返回其更新版本作为输出。\n",
    "\n",
    "例如，`plan_node` 将填充 `plan` 字段，`retrieval_node` 将填充 `retrieved_docs` 字段，依此类推。这种共享的持久状态使得我们简单的 RAG 链能够进行复杂的迭代推理，而这正是我们简单的 RAG 链所缺乏的。\n",
    "\n",
    "现在，我们已经定义了 agent 的记忆蓝图，可以构建系统的第一个认知组件：填充此状态的规划 agent。\n",
    "\n",
    "## 六、战略规划和查询制定\n",
    "\n",
    "定义好 RAGState 后，我们现在可以构建 agent 的第一个、可以说是最关键的认知组件：规划能力，这正是我们的系统从简单的数据获取器跃升为真正的推理引擎的地方。我们的 agent 不会简单地将用户的复杂查询视为单一搜索，而是会先暂停、思考，然后构建一个详细的、循序渐进的研究策略。\n",
    "\n",
    "本节分为三个关键工程步骤：\n",
    "\n",
    "- **工具感知规划器**：我们将构建一个由 LLM 驱动的 agent，其唯一工作是将用户的查询分解为结构化的 Plan 对象，并决定每个步骤使用哪种工具。\n",
    "- **查询重写器**：我们将创建一个专门的 agent，将规划器的简单子问题转换为高效、优化的搜索查询。\n",
    "- **元数据感知分块**：我们将重新处理源文档以添加部分级元数据，这是实现高精度、过滤检索的关键步骤。\n",
    "\n",
    "### 6.1 使用工具感知规划器分解问题\n",
    "\n",
    "我们要构建我们的行动大脑，当这个大脑遇到一个复杂的问题时，它需要做的第一件事就是制定一个任务规划计划。\n",
    "\n",
    "为此，我们将创建一个专用的规划 agent，我们需要给它一组非常清晰的指令或提示，明确地告诉它它的工作是什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "172afe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from rich.pretty import pprint as rprint\n",
    "\n",
    "# The system prompt that instructs the LLM how to behave as a planner\n",
    "planner_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are an expert research planner. Your task is to create a clear, multi-step plan \n",
    "        to answer a complex user query by retrieving information from multiple sources.\n",
    "\n",
    "        You have two tools available:\n",
    "        1. `search_10k`: Use this to search for information within NVIDIA's 2023 10-K financial filing. \n",
    "           This is best for historical facts, financial data, and stated company policies or risks \n",
    "           from that specific time period.\n",
    "        2. `search_web`: Use this to search the public internet for recent news, competitor information, \n",
    "           or any topic that is not specific to NVIDIA's 2023 10-K.\n",
    "\n",
    "        Decompose the user's query into a series of simple, sequential sub-questions. \n",
    "        For each step, decide which tool is more appropriate.\n",
    "        For `search_10k` steps, also identify the most likely section of the 10-K \n",
    "        (e.g., 'Item 1A. Risk Factors', 'Item 7. Management's Discussion and Analysis...').\n",
    "        It is critical to use the exact section titles found in a 10-K filing where possible.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"User Query: {question}\"\n",
    "    )  # The user's original, complex query\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba858b",
   "metadata": {},
   "source": [
    "我们实际上是在赋予 LLM 一个新的角色：一位专家级的研究规划师。我们明确地告诉它可以使用的两个工具（`search_10k` 和 `search_web`），并指导它何时使用它们。这就是\"工具感知\"的部分。\n",
    "\n",
    "我们不仅要求它制定计划，还要求它创建一个直接映射到我们已经构建的能力的计划。\n",
    "\n",
    "现在我们可以启动推理模型并将其与提示链接在一起，这里非常重要的一步是告诉 LLM 它的最终输出必须采用 Pydantic Plan 类的格式，使得输出结构化且可预测。\n",
    "\n",
    "我们将 `planner_prompt` 传递给强大的 `reasoning_llm`，然后使用 `.with_structured_output(Plan)` 方法。这会告诉 LangChain 使用模型函数调用功能将其响应格式化为与 Plan Pydantic 模式完美匹配的 JSON 对象。这比尝试解析纯文本响应要可靠得多。\n",
    "\n",
    "让我们看看使用挑战查询进行测试时的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa67ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool-Aware Planner Agent created successfully.\n",
      "\n",
      "--- Testing Planner Agent ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Plan</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">steps</span>=<span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Step</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">sub_question</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"What are the key risks related to competition mentioned in NVIDIA's 2023 10-K filing?\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">justification</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"To understand NVIDIA's competitive landscape and risks, we need to identify what the company itself has stated as risks related to competition in their official financial filing.\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'search_10k'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">keywords</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'competition'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'competitive risks'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'market competition'</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">document_section</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Item 1A. Risk Factors'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Step</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">sub_question</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"What is AMD's AI chip strategy as of 2024?\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">justification</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"To assess how AMD's strategy might impact NVIDIA, we need to understand AMD's current approach and developments in AI chips.\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'search_web'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">keywords</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'AMD AI chip strategy 2024'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'AMD AI developments'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'AMD competition NVIDIA'</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">document_section</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Step</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">sub_question</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"How does AMD's 2024 AI chip strategy address or exacerbate one of NVIDIA's stated competitive risks?\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">justification</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"By comparing AMD's strategy with NVIDIA's stated risks, we can determine the potential impact on NVIDIA's competitive position.\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'search_web'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">keywords</span>=<span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'AMD AI strategy impact on NVIDIA'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'NVIDIA competitive risks AMD'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'NVIDIA AMD competition'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">document_section</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPlan\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33msteps\u001b[0m=\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33msub_question\u001b[0m=\u001b[32m\"What\u001b[0m\u001b[32m are the key risks related to competition mentioned in NVIDIA's 2023 10-K filing?\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mjustification\u001b[0m=\u001b[32m\"To\u001b[0m\u001b[32m understand NVIDIA's competitive landscape and risks, we need to identify what the company itself has stated as risks related to competition in their official financial filing.\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mtool\u001b[0m=\u001b[32m'search_10k'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mkeywords\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'competition'\u001b[0m, \u001b[32m'competitive risks'\u001b[0m, \u001b[32m'market competition'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mdocument_section\u001b[0m=\u001b[32m'Item 1A. Risk Factors'\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33msub_question\u001b[0m=\u001b[32m\"What\u001b[0m\u001b[32m is AMD's AI chip strategy as of 2024?\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mjustification\u001b[0m=\u001b[32m\"To\u001b[0m\u001b[32m assess how AMD's strategy might impact NVIDIA, we need to understand AMD's current approach and developments in AI chips.\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mtool\u001b[0m=\u001b[32m'search_web'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mkeywords\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'AMD AI chip strategy 2024'\u001b[0m, \u001b[32m'AMD AI developments'\u001b[0m, \u001b[32m'AMD competition NVIDIA'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mdocument_section\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33msub_question\u001b[0m=\u001b[32m\"How\u001b[0m\u001b[32m does AMD's 2024 AI chip strategy address or exacerbate one of NVIDIA's stated competitive risks?\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mjustification\u001b[0m=\u001b[32m\"By\u001b[0m\u001b[32m comparing AMD's strategy with NVIDIA's stated risks, we can determine the potential impact on NVIDIA's competitive position.\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mtool\u001b[0m=\u001b[32m'search_web'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mkeywords\u001b[0m=\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   │   │   │   \u001b[0m\u001b[32m'AMD AI strategy impact on NVIDIA'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   │   \u001b[0m\u001b[32m'NVIDIA competitive risks AMD'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   │   \u001b[0m\u001b[32m'NVIDIA AMD competition'\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mdocument_section\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize our powerful reasoning model, as defined in the config\n",
    "reasoning_llm = ChatOpenAI(model=config[\"reasoning_llm\"], temperature=0)\n",
    "\n",
    "# Create the planner agent by piping the prompt to the LLM and instructing it to use our structured 'Plan' output\n",
    "planner_agent = planner_prompt | reasoning_llm.with_structured_output(Plan)\n",
    "print(\"Tool-Aware Planner Agent created successfully.\")\n",
    "\n",
    "# Let's test the planner agent with our complex query to see its output\n",
    "print(\"\\n--- Testing Planner Agent ---\")\n",
    "test_plan = planner_agent.invoke({\"question\": complex_query_adv})\n",
    "\n",
    "# Use rich's pretty print for a clean, readable display of the Pydantic object\n",
    "rprint(test_plan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef52cd5",
   "metadata": {},
   "source": [
    "如果我们查看输出，就会发现 agent 不仅仅给出了一个模糊的计划，它还生成了一个结构化的 Plan 对象。它正确地识别出该查询包含两个部分。对于第一部分，它知道答案在 10-K 中并选择了 `search_10k` 工具，甚至正确猜出了正确的文档部分。对于第二部分，它知道\"2024 年的新闻\"不可能出现在 2023 年的文档中，并正确地选择了 `search_web` 工具。这标志着我们的管道至少在思考过程中能够给出令人满意的结果。\n",
    "\n",
    "### 6.2 使用查询重写代理优化检索\n",
    "\n",
    "所以，基本上我们有一个包含良好子问题的计划。但像\"风险是什么？\"这样的问题并不是一个很好的搜索查询。它太笼统了。搜索引擎，无论是矢量数据库还是网页搜索，最适合处理具体的、关键词丰富的查询。\n",
    "\n",
    "为了解决这个问题，我们将构建另一个小型的专用 agent：查询重写器。它的唯一工作就是获取当前步骤的子问题，并通过从我们已经学到的知识中添加相关的关键字和上下文，使其更适合搜索。\n",
    "\n",
    "首先，让我们为这个新代理设计提示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "046a39c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser  # To parse the LLM's output as a simple string\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# The prompt for our query rewriter, instructing it to act as a search expert\n",
    "query_rewriter_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are a search query optimization expert. Your task is to rewrite a given sub-question\n",
    "        into a highly effective search query for a vector database or web search engine, using keywords \n",
    "        and context from the research plan.\n",
    "\n",
    "        The rewritten query should be specific, use terminology likely to be found in the target source \n",
    "        (a financial 10-K or news articles), and be structured to retrieve the most relevant text snippets.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"Current sub-question: {sub_question}\\n\\n\"\n",
    "        \"Relevant keywords from plan: {keywords}\\n\\n\"\n",
    "        \"Context from past steps:\\n{past_context}\"\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d02563a",
   "metadata": {},
   "source": [
    "我们基本上是在告诉这个 agent，让它像一个搜索查询优化专家一样工作。我们给它提供了三条信息：简单的 `sub_question`、我们的规划器已经识别出的 `keywords`，以及来自任何先前研究步骤的 `past_context`。这为它提供了构建更优查询所需的所有原始材料。\n",
    "\n",
    "现在我们可以启动这个 agent 了。这是一个简单的链，因为我们只需要一个字符串作为输出。\n",
    "\n",
    "为了正确测试这一点，我们必须模拟一个真实的场景。我们创建一个 `test_past_context` 字符串，它代表 agent 在其计划的前两个步骤中已经生成的摘要。然后，我们将它和下一个子问题一起输入到 `query_rewriter_agent` 中。\n",
    "\n",
    "我们来看看结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e66d927b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Rewriter Agent created successfully.\n",
      "\n",
      "--- Testing Query Rewriter Agent ---\n",
      "Original sub-question: How does AMD's 2024 AI chip strategy potentially exacerbate the competitive risks identified in NVIDIA's 10-K?\n",
      "Rewritten Search Query: \"AMD 2024 AI chip strategy impact on NVIDIA competitive risks 10-K, MI300X vs H100, market share threat, technological change pressure\"\n"
     ]
    }
   ],
   "source": [
    "# Create the agent by piping the prompt to our reasoning LLM and a string output parser\n",
    "query_rewriter_agent = query_rewriter_prompt | reasoning_llm | StrOutputParser()\n",
    "print(\"Query Rewriter Agent created successfully.\")\n",
    "\n",
    "# Let's test the rewriter agent. We'll pretend we've already completed the first two steps of our plan.\n",
    "print(\"\\n--- Testing Query Rewriter Agent ---\")\n",
    "\n",
    "# Test sub-question: final synthesis-style query\n",
    "test_sub_q = (\n",
    "    \"How does AMD's 2024 AI chip strategy potentially exacerbate the competitive \"\n",
    "    \"risks identified in NVIDIA's 10-K?\"\n",
    ")\n",
    "\n",
    "# Relevant keywords from the hypothetical plan\n",
    "test_keywords = [\n",
    "    \"impact\",\n",
    "    \"threaten\",\n",
    "    \"competitive pressure\",\n",
    "    \"market share\",\n",
    "    \"technological change\",\n",
    "]\n",
    "\n",
    "# Mock past context, simulating prior steps' findings\n",
    "test_past_context = (\n",
    "    \"Step 1 Summary: NVIDIA's 10-K lists intense competition and rapid technological \"\n",
    "    \"change as key risks. \"\n",
    "    \"Step 2 Summary: AMD launched its MI300X AI accelerator in 2024 to directly compete \"\n",
    "    \"with NVIDIA's H100.\"\n",
    ")\n",
    "\n",
    "# Invoke the agent with our test data\n",
    "rewritten_q = query_rewriter_agent.invoke(\n",
    "    {\n",
    "        \"sub_question\": test_sub_q,\n",
    "        \"keywords\": test_keywords,\n",
    "        \"past_context\": test_past_context,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Original sub-question: {test_sub_q}\")\n",
    "print(f\"Rewritten Search Query: {rewritten_q}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c5f13e",
   "metadata": {},
   "source": [
    "最初的问题是针对分析师的，他重写了查询语句，用于搜索引擎。它被分配了特定的术语，例如 \"MI300X\"、\"market share erosion\" 和 \"data center\"，所有这些术语都是根据关键词和过去的上下文综合而成的。\n",
    "\n",
    "这样的查询更有可能检索到正确的文档，从而使我们的整个系统更加准确和高效。这个重写步骤将成为我们主代理循环的关键部分。\n",
    "\n",
    "### 6.3 元数据感知分块的精度\n",
    "\n",
    "规划 agent 给了我们一个很好的机会。它不仅仅是告诉我们\"发现风险\"，还给了我们一个提示：在项目 1A 的\"风险因素\"部分中查找风险。但目前，我们的检索器无法使用该提示。我们的向量存储只是一个包含 378 个文本块的扁平大列表。它根本不知道\"section\"是什么。\n",
    "\n",
    "我们需要解决这个问题。我们将从头开始重建文档块。这次，对于我们创建的每个块，我们将添加一个标签或标记，用于标记其元数据，以便系统准确地告知它来自 10-K 文档的哪个部分。这将使我们的代理稍后能够执行高度精确的过滤搜索。\n",
    "\n",
    "首先，我们需要一种编程方式来查找原始文本文件中每个部分的起始位置。查看文档，我们可以看到一个清晰的模式：每个主要部分都以单词 \"ITEM\" 开头，后跟一个数字，例如 \"ITEM 1A\" 或 \"ITEM 7\"。这对于正则表达式来说非常完美。\n",
    "\n",
    "我们基本上是在创建一个模式，作为我们的文本段检测器。它应该被设计得足够灵活，能够捕捉不同的格式，同时又足够具体，不会抓取错误的文本。\n",
    "\n",
    "现在我们可以使用此模式将文档分成两个单独的列表：一个仅包含章节标题，另一个包含每个章节内的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02325903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified 22 document sections.\n",
      "Section content blocks: 22\n",
      "- ITEM 1. BUSINESS\n",
      "- ITEM 1A. RISK FACTORS\n",
      "- ITEM 1B. UNRESOLVED STAFF COMMENTS\n",
      "- ITEM 2. PROPERTIES\n",
      "- ITEM 3. LEGAL PROCEEDINGS\n",
      "- ITEM 4. MINE SAFETY DISCLOSURES\n",
      "- ITEM 5. MARKET FOR REGISTRANT’S COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES\n",
      "- ITEM 6. [RESERVED]\n",
      "- ITEM 7. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "- ITEM 7A. QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\n",
      "- ITEM 8. FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "- ITEM 9. CHANGES IN AND DISAGREEMENTS WITH ACCOUNTANTS ON ACCOUNTING AND FINANCIAL DISCLOSURE\n",
      "- ITEM 9A. CONTROLS AND PROCEDURES\n",
      "- ITEM 9B. OTHER INFORMATION\n",
      "- ITEM 9C. DISCLOSURE REGARDING FOREIGN JURISDICTIONS THAT PREVENT INSPECTIONS\n",
      "- ITEM 10. DIRECTORS, EXECUTIVE OFFICERS AND CORPORATE GOVERNANCE\n",
      "- ITEM 11. EXECUTIVE COMPENSATION\n",
      "- ITEM 12. SECURITY OWNERSHIP OF CERTAIN BENEFICIAL OWNERS AND MANAGEMENT AND RELATED STOCKHOLDER MATTERS\n",
      "- ITEM 13. CERTAIN RELATIONSHIPS AND RELATED TRANSACTIONS, AND DIRECTOR INDEPENDENCE\n",
      "- ITEM 14. PRINCIPAL ACCOUNTANT FEES AND SERVICES\n",
      "- ITEM 15. EXHIBIT AND FINANCIAL STATEMENT SCHEDULES\n",
      "- ITEM 16. FORM 10-K SUMMARY\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "raw_text = documents[0].page_content\n",
    "\n",
    "# 只匹配独立一行的 ITEM 标题：\n",
    "# - 行首可有空格\n",
    "# - ITEM + 数字(+可选字母) + '.' \n",
    "# - 后面标题内容不包含句号，一直到行尾（防止把整句正文吃进来）\n",
    "section_header_pattern = re.compile(\n",
    "    r\"\"\"\n",
    "    ^\\s*                                  # 行首空格\n",
    "    ITEM\\s+(?P<num>\\d+[A-Z]?)\\.\\s+        # ITEM 1. / ITEM 1A.\n",
    "    (?P<title>[^.\\n]{1,200}?)\\s*          # 标题：无句号，不跨行，长度限制防止乱飙\n",
    "    $                                     # 必须到行尾结束\n",
    "    \"\"\",\n",
    "    re.IGNORECASE | re.MULTILINE | re.VERBOSE\n",
    ")\n",
    "\n",
    "matches = list(section_header_pattern.finditer(raw_text))\n",
    "\n",
    "section_titles = []\n",
    "sections_content = []\n",
    "\n",
    "for i, m in enumerate(matches):\n",
    "    num = m.group(\"num\").upper()\n",
    "    title = m.group(\"title\").strip()\n",
    "\n",
    "    clean_title = f\"ITEM {num}. {title}\"\n",
    "    section_titles.append(clean_title)\n",
    "\n",
    "    start = m.end()\n",
    "    end = matches[i + 1].start() if i + 1 < len(matches) else len(raw_text)\n",
    "    content = raw_text[start:end].strip()\n",
    "    sections_content.append(content)\n",
    "\n",
    "print(f\"Identified {len(section_titles)} document sections.\")\n",
    "print(f\"Section content blocks: {len(sections_content)}\")\n",
    "for t in section_titles:\n",
    "    print(f\"- {t}\")\n",
    "\n",
    "assert len(section_titles) == len(sections_content), \"Mismatch between titles and content sections\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f07cd5",
   "metadata": {},
   "source": [
    "这是一种解析半结构化文档的非常有效的方法。我们使用了两次正则表达式：一次是获取所有章节标题的清晰列表，另一次是将正文拆分为内容块列表。assert 语句让我们确信解析逻辑是合理的。\n",
    "\n",
    "好了，现在我们有了这些部分：一个标题列表和一个对应的内容列表。现在我们可以循环遍历它们，并创建最终的、包含丰富元数据的块。\n",
    "\n",
    "这是我们升级的核心。我们逐一迭代每个部分。对于每个部分，我们创建文本块。但在将它们添加到最终列表之前，我们会创建一个 metadata 字典并附加 section_title。这有效地为每个块标记了其来源。\n",
    "\n",
    "让我们看一下输出并看看有什么区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e54c19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 368 chunks with section metadata.\n",
      "\n",
      "--- Sample Chunk with Metadata ---\n",
      "page_content='In evaluating NVIDIA, the following risk factors should be considered in addition to the other information in this Annual Report on Form 10-K. Purchasing or owning NVIDIA common stock involves investment risks including, but not limited to, the risks described below. Any one of the following risks could harm our business, financial condition, results of operations or reputation, which could cause our stock price to decline, and you may lose all or a part of your investment. Additional risks, trends and uncertainties not presently known to us or that we currently believe are immaterial may also harm our business, financial condition, results of operations or reputation.\n",
      "\n",
      "Risk Factors Summary\n",
      "\n",
      "Risks Related to Our Industry and Markets\n",
      "\n",
      "• Failure to meet the evolving needs of our industry and markets may adversely impact our financial results.\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "• Competition in our current and target markets could cause us to lose market share and revenue.' metadata={'section': 'ITEM 1A. RISK FACTORS', 'source_doc': 'c:\\\\Users\\\\22959\\\\Documents\\\\WXSK_Works\\\\Agentic RAG Project\\\\data\\\\nvda_10k_clean.txt', 'id': '1f0d7347-530b-43fb-93d1-952728271b36'}\n"
     ]
    }
   ],
   "source": [
    "import uuid  # We'll use this to give each chunk a unique ID, which is good practice\n",
    "\n",
    "# This list will hold our new, metadata-rich document chunks\n",
    "doc_chunks_with_metadata = []\n",
    "\n",
    "# Loop through each section's content along with its title using enumerate\n",
    "for i, content in enumerate(sections_content):\n",
    "    # Get the corresponding title for the current content block\n",
    "    section_title = section_titles[i]\n",
    "\n",
    "    # Use the same text splitter as before, but this time, we run it ONLY on the content of the current section\n",
    "    section_chunks = text_splitter.split_text(content)\n",
    "\n",
    "    # Now, loop through the smaller chunks created from this one section\n",
    "    for chunk in section_chunks:\n",
    "        # Generate a unique ID for this specific chunk\n",
    "        chunk_id = str(uuid.uuid4())\n",
    "\n",
    "        # Create a new LangChain Document object for the chunk\n",
    "        doc_chunks_with_metadata.append(\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                # This is the most important part: we attach the metadata\n",
    "                metadata={\n",
    "                    \"section\": section_title,       # The section this chunk belongs to\n",
    "                    \"source_doc\": doc_path_clean,   # Where the document came from\n",
    "                    \"id\": chunk_id                  # The unique ID for this chunk\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"Created {len(doc_chunks_with_metadata)} chunks with section metadata.\")\n",
    "print(\"\\n--- Sample Chunk with Metadata ---\")\n",
    "\n",
    "# To prove it worked, let's find a chunk that we know should be in the 'Risk Factors' section and print it\n",
    "sample_chunk = next(\n",
    "    c\n",
    "    for c in doc_chunks_with_metadata\n",
    "    if \"risk factors\" in c.metadata.get(\"section\", \"\").lower()\n",
    ")\n",
    "print(sample_chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcc96f9",
   "metadata": {},
   "source": [
    "看看那个 metadata 块。我们之前提到的那段文本现在附加了一段上下文：`'section': 'Item 1A. Risk Factors.'`。\n",
    "\n",
    "现在，当我们的 agent 需要查找风险时，它可以告诉检索器：\"嘿，不要搜索所有 381 个块。只需搜索部分元数据为'项目 1A. 风险因素'的块即可。\"\n",
    "\n",
    "这个简单的改变将我们的检索器从钝器转变为手术工具，这是构建真正生产级 RAG 系统的关键原则。\n",
    "\n",
    "## 七、创建多阶段检索漏斗\n",
    "\n",
    "到目前为止，我们已经设计了一个智能规划器，并用元数据丰富了我们的文档。现在，我们准备构建系统的核心：一个复杂的检索管道。\n",
    "\n",
    "简单的一次性语义搜索已经不够了。对于生产级代理，我们需要一个自适应且多阶段的检索过程。\n",
    "\n",
    "我们将把检索过程设计成一个漏斗，其中每个阶段都会细化前一个阶段的结果：\n",
    "\n",
    "- **检索 Supervisor**：构建一个新的 Supervisor 代理，充当动态路由器，分析每个子问题并选择最佳搜索策略（向量、关键字或混合）。\n",
    "- **第 1 阶段（广泛回忆）**：Supervisor 可以选择的不同检索策略，重点是广泛收集所有可能相关的文档。\n",
    "- **第 2 阶段（高精度）**：使用 Cross-Encoder 模型对初始结果进行重新排序，丢弃噪音并将最相关的文档排序到最前面。\n",
    "- **第 3 阶段（综合）**：最后，我们将创建一个 Distiller Agent，将排名靠前的文档压缩成一个简洁的段落，供我们的下游 agent 使用。\n",
    "\n",
    "### 7.1 使用 Supervisor 动态选择策略\n",
    "\n",
    "并非所有搜索查询都是一样的。像\"计算和网络\"部分的收入是多少？这样的问题包含具体、精确的术语。基于关键词的搜索非常适合这种情况。\n",
    "\n",
    "公司对市场竞争的看法是什么？这还是个概念问题。基于语义向量的搜索会更好。\n",
    "\n",
    "我们不会采用硬编码一个策略，而是构建一个小型智能代理——检索主管 (Retrieval Supervisor)，来为我们做出这个决定。它唯一的工作就是查看搜索查询，并决定哪种检索方法最合适。\n",
    "\n",
    "首先，我们需要定义主管可能做出的决策。我们将使用 Pydantic BaseModel 来构建其输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82d625f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class RetrievalDecision(BaseModel):\n",
    "    # The chosen retrieval strategy. Must be one of these three options.\n",
    "    strategy: Literal[\"vector_search\", \"keyword_search\", \"hybrid_search\"]\n",
    "    # The agent's justification for its choice.\n",
    "    justification: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0c39d",
   "metadata": {},
   "source": [
    "监管者必须从这三种策略中选择一种，并解释其理由。这使得其决策过程透明可靠。\n",
    "\n",
    "现在，让我们创建指导该代理行为的提示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67304e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_supervisor_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are a retrieval strategy expert. Based on the user's query, you must decide the best retrieval strategy.\n",
    "You have three options:\n",
    "1. `vector_search`: Best for conceptual, semantic, or similarity-based queries.\n",
    "2. `keyword_search`: Best for queries with specific, exact terms, names, or codes (e.g., 'Item 1A', 'Hopper architecture').\n",
    "3. `hybrid_search`: A good default that combines both, but may be less precise than a targeted strategy.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"User Query: {sub_question}\"  # The rewritten search query will be passed here.\n",
    "    ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cb8384",
   "metadata": {},
   "source": [
    "我们在这里创建了一个非常直接的提示，告诉 LLM 它的角色是检索策略专家，并清楚地解释其每个可用策略何时最有效。\n",
    "\n",
    "最后，我们可以组装我们的主管代理。\n",
    "\n",
    "使用 `.with_structured_output(RetrievalDecision)` 把所有东西连在一起，确保我们从 LLM 返回一个干净、可预测的 RetrievalDecision 对象。让我们看看测试结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e008a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Supervisor Agent created.\n",
      "\n",
      "--- Testing Retrieval Supervisor Agent ---\n",
      "Query: 'revenue growth for the Compute & Networking segment in fiscal year 2023'\n",
      "Decision: keyword_search, Justification: The query is looking for specific information related to 'revenue growth', 'Compute & Networking segment', and 'fiscal year 2023'. These are precise terms that are likely to be found in financial reports or documents where exact matches are important. Therefore, a keyword search is the most appropriate strategy to retrieve accurate and relevant information.\n",
      "\n",
      "Query: 'general sentiment about market competition and technological innovation'\n",
      "Decision: vector_search, Justification: The query is conceptual and seeks an understanding of general sentiment, which involves interpreting nuanced and broad ideas about market competition and technological innovation. Vector search is best suited for capturing the semantic meaning and context of such queries.\n"
     ]
    }
   ],
   "source": [
    "# Create the agent by piping our prompt to the reasoning LLM and structuring its output with our Pydantic class\n",
    "retrieval_supervisor_agent = retrieval_supervisor_prompt | reasoning_llm.with_structured_output(RetrievalDecision)\n",
    "print(\"Retrieval Supervisor Agent created.\")\n",
    "\n",
    "# Let's test it with two different types of queries to see how it behaves\n",
    "print(\"\\n--- Testing Retrieval Supervisor Agent ---\")\n",
    "\n",
    "query1 = \"revenue growth for the Compute & Networking segment in fiscal year 2023\"\n",
    "decision1 = retrieval_supervisor_agent.invoke({\"sub_question\": query1})\n",
    "print(f\"Query: '{query1}'\")\n",
    "print(f\"Decision: {decision1.strategy}, Justification: {decision1.justification}\")\n",
    "\n",
    "query2 = \"general sentiment about market competition and technological innovation\"\n",
    "decision2 = retrieval_supervisor_agent.invoke({\"sub_question\": query2})\n",
    "print(f\"\\nQuery: '{query2}'\")\n",
    "print(f\"Decision: {decision2.strategy}, Justification: {decision2.justification}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b419ed",
   "metadata": {},
   "source": [
    "我们可以看到，它正确识别出第一个充满特定术语的查询并选择了 `keyword_search`。\n",
    "\n",
    "对于第二个概念性抽象的查询，正确地选择了 `vector_search`。这种在检索漏斗开始时就进行动态决策的做法，比起千篇一律的做法，无疑是一次很好的升级。\n",
    "\n",
    "### 7.2 通过混合搜索、关键字和语义搜索进行广泛召回\n",
    "\n",
    "现在我们有了主管来选择策略，我们需要构建检索策略本身。我们漏斗的第一阶段是关于召回的，我们的目标是广撒网，捕获所有可能相关的文档，即使我们在此过程中会发现一些噪音。\n",
    "\n",
    "为此，我们将实现可以调用的三个不同的搜索功能：\n",
    "\n",
    "- **矢量搜索**：我们的标准语义搜索，但现在升级为使用元数据过滤器。\n",
    "- **关键词搜索（BM25）**：一种经典、强大的算法，擅长查找具有特定、精确术语的文档。\n",
    "- **混合搜索**：两种方法中最好的一种是使用称为倒数秩融合（RRF）的技术将向量和关键字搜索的结果结合起来。\n",
    "\n",
    "首先，我们需要使用上一节中创建的富含元数据的块来创建一个新的高级矢量存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "182b8e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced vector store with metadata...\n",
      "Advanced vector store created with 727 embeddings.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # A fundamental library for numerical operations in Python\n",
    "from rank_bm25 import BM25Okapi  # The library for implementing the BM25 keyword search algorithm\n",
    "\n",
    "print(\"Creating advanced vector store with metadata...\")\n",
    "# We create a new Chroma vector store, this time using our metadata-rich chunks\n",
    "advanced_vector_store = Chroma.from_documents(\n",
    "    documents=doc_chunks_with_metadata,\n",
    "    embedding=embedding_function)\n",
    "print(f\"Advanced vector store created with {advanced_vector_store._collection.count()} embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d826ac0",
   "metadata": {},
   "source": [
    "这是一个简单但关键的步骤。现在，`advanced_vector_store` 包含与基准相同的文本，但每个嵌入的块都带有其章节标题的标签，从而解锁了我们执行过滤搜索的能力。\n",
    "\n",
    "接下来，我们需要为关键词搜索做准备。BM25 算法的工作原理是分析文档中单词的频率。为了实现这一点，我们需要对语料库进行预处理，将每个文档的内容拆分成一个单词列表（token）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca848b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building BM25 index for keyword search...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBuilding BM25 index for keyword search...\")\n",
    "# Create a list where each element is a list of words from a document\n",
    "tokenized_corpus = [doc.page_content.split(\" \") for doc in doc_chunks_with_metadata]\n",
    "# Create a list of all unique document IDs\n",
    "doc_ids = [doc.metadata[\"id\"] for doc in doc_chunks_with_metadata]\n",
    "# Create a mapping from a document's ID back to the full Document object for easy lookup\n",
    "doc_map = {doc.metadata[\"id\"]: doc for doc in doc_chunks_with_metadata}\n",
    "# Initialize the BM25Okapi index with our tokenized corpus\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8918c",
   "metadata": {},
   "source": [
    "我们基本上是在为 BM25 索引创建必要的数据结构。`tokenized_corpus` 是要搜索的内容，而 `doc_map` 则允许我们在搜索完成后快速检索完整的 Document 对象。\n",
    "\n",
    "现在我们可以定义三个检索函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a5e629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All retrieval strategy functions ready.\n"
     ]
    }
   ],
   "source": [
    "# Strategy 1: Pure Vector Search with Metadata Filtering\n",
    "def vector_search_only(query: str, section_filter: str = None, k: int = 10):\n",
    "    # This dictionary defines the metadata filter. ChromaDB will only search documents that match this.\n",
    "    filter_dict = {\"section\": section_filter} if section_filter and \"Unknown\" not in section_filter else None\n",
    "    # Perform the similarity search with the optional filter\n",
    "    return advanced_vector_store.similarity_search(query, k=k, filter=filter_dict)\n",
    "\n",
    "# Strategy 2: Pure Keyword Search (BM25)\n",
    "def bm25_search_only(query: str, k: int = 10):\n",
    "    # Tokenize the incoming query\n",
    "    tokenized_query = query.split(\" \")\n",
    "    # Get the BM25 scores for the query against all documents in the corpus\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    # Get the indices of the top k documents\n",
    "    top_k_indices = np.argsort(bm25_scores)[::-1][:k]\n",
    "    # Use our doc_map to return the full Document objects for the top results\n",
    "    return [doc_map[doc_ids[i]] for i in top_k_indices]\n",
    "\n",
    "# Strategy 3: Hybrid Search with Reciprocal Rank Fusion (RRF)\n",
    "def hybrid_search(query: str, section_filter: str = None, k: int = 10):\n",
    "    # 1. Perform a keyword search\n",
    "    bm25_docs = bm25_search_only(query, k=k)\n",
    "    # 2. Perform a semantic search with the metadata filter\n",
    "    semantic_docs = vector_search_only(query, section_filter=section_filter, k=k)\n",
    "    # 3. Combine and re-rank the results using Reciprocal Rank Fusion (RRF)\n",
    "    # Get a unique set of all documents found by either search method\n",
    "    all_docs = {doc.metadata[\"id\"]: doc for doc in bm25_docs + semantic_docs}.values()\n",
    "    # Create lists of just the document IDs from each search result\n",
    "    ranked_lists = [[doc.metadata[\"id\"] for doc in bm25_docs], [doc.metadata[\"id\"] for doc in semantic_docs]]\n",
    "    \n",
    "    # Initialize a dictionary to store the RRF scores for each document\n",
    "    rrf_scores = {}\n",
    "    # Loop through each ranked list (BM25 and Semantic)\n",
    "    for doc_list in ranked_lists:\n",
    "        # Loop through each document ID in the list with its rank (i)\n",
    "        for i, doc_id in enumerate(doc_list):\n",
    "            if doc_id not in rrf_scores:\n",
    "                rrf_scores[doc_id] = 0\n",
    "            # The RRF formula: add 1 / (rank + k) to the score. We use k=61 as a standard default.\n",
    "            rrf_scores[doc_id] += 1 / (i + 61)\n",
    "    \n",
    "    # Sort the document IDs based on their final RRF scores in descending order\n",
    "    sorted_doc_ids = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)\n",
    "    # Return the top k Document objects based on the fused ranking\n",
    "    final_docs = [doc_map[doc_id] for doc_id in sorted_doc_ids[:k]]\n",
    "    return final_docs\n",
    "print(\"\\nAll retrieval strategy functions ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba273f",
   "metadata": {},
   "source": [
    "我们现在已经实现了自适应检索系统的核心。`vector_search_only` 函数是我们升级的语义搜索函数。关键新增功能是 `filter=filter_dict` 参数，它允许我们从规划器的 Step 中传递 `document_section`，并强制搜索仅考虑包含该元数据的块。\n",
    "\n",
    "`bm25_search_only` 函数是我们的纯关键词检索器。它能够快速有效地查找语义搜索可能遗漏的特定术语。\n",
    "\n",
    "`hybrid_search` 函数并行运行两个搜索，然后使用 RRF 智能地合并结果。RRF 是一种简单但功能强大的算法，它根据文档在每个列表中的位置对其进行排序，从而有效地赋予在两个搜索结果中排名靠前的文档更高的权重。\n",
    "\n",
    "让我们快速测试一下关键词搜索的效果。我们将搜索规划师确定的确切章节标题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0901a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Keyword Search ---\n",
      "Query: Item 1A. Risk Factors\n",
      "Found 10 documents.\n",
      "- ITEM 1. BUSINESS (ID: 66cbfbd9-6de0-4005-b742-74c8cf6b57aa)\n",
      "- ITEM 1. BUSINESS (ID: 65aad02d-8163-480d-9e92-2e646fc1b1f3)\n",
      "- ITEM 1. BUSINESS (ID: 611810a3-6997-499e-8227-754a213dc994)\n",
      "- ITEM 7. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS (ID: 0b72ce76-859c-4aca-9ce7-1e83cb7e796e)\n",
      "- ITEM 7. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS (ID: daa911ed-e771-4b5c-bed8-518aa2a119d0)\n",
      "- ITEM 8. FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA (ID: b6f73d18-d0fe-4801-9498-86640bdb76c6)\n",
      "- ITEM 1A. RISK FACTORS (ID: 1f0d7347-530b-43fb-93d1-952728271b36)\n",
      "- ITEM 3. LEGAL PROCEEDINGS (ID: db5e37be-d72a-4a4f-9df0-805ce8779867)\n",
      "- ITEM 7. MANAGEMENT’S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS (ID: e65936df-1c27-4613-9819-273093e508b5)\n",
      "- ITEM 5. MARKET FOR REGISTRANT’S COMMON EQUITY, RELATED STOCKHOLDER MATTERS AND ISSUER PURCHASES OF EQUITY SECURITIES (ID: 132f448f-058a-47f4-beed-742eedab761d)\n"
     ]
    }
   ],
   "source": [
    "# Test Keyword Search to see if it can precisely find a specific section\n",
    "print(\"\\n--- Testing Keyword Search ---\")\n",
    "test_query = \"Item 1A. Risk Factors\"\n",
    "test_results = bm25_search_only(test_query)\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Found {len(test_results)} documents.\")\n",
    "for res in test_results:\n",
    "    print(f\"- {res.metadata['section']} (ID: {res.metadata['id']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed7d5b",
   "metadata": {},
   "source": [
    "输出结果正是我们想要的。BM25 搜索以关键词为中心，只需搜索标题，就能完美且快速地检索到 \"Item 1A. Risk Factors\" 部分的文档。\n",
    "\n",
    "当查询包含特定关键字（例如章节标题）时，我们的主管现在可以选择这个精确的工具。\n",
    "\n",
    "随着我们广泛的召回阶段的构建，我们拥有了一个强大的机制来查找所有可能相关的文档。然而，这张广袤的网络也可能带来不相关的噪音。我们漏斗的下一阶段将专注于高精度地过滤这些噪音。\n",
    "\n",
    "### 7.3 使用 Cross-Encoder 重排序器实现高精度\n",
    "\n",
    "我们的第一阶段检索在召回率方面做得很好，提取了 10 篇可能与我们的子问题相关的文档。\n",
    "\n",
    "但问题是，它们只是潜在相关的。将这 10 个文档直接输入到我们的推理 LLM，效率低下且风险很大。它会增加令牌成本，更重要的是，它会用嘈杂的、半相关的信息混淆模型。\n",
    "\n",
    "我们现在需要的是\"精准\"阶段。我们需要一种方法来检查这 10 篇候选文档，并挑选出绝对最佳的文档。这时，重排器就派上用场了。关键的区别在于这些模型的工作方式。我们的初始检索使用双编码器（嵌入模型），它分别针对查询和文档创建向量。它速度很快，非常适合搜索数百万个项目。重排序的交叉编码器将查询和单个文档作为一对，进行更深入、更细致的比较。它速度较慢，但准确率更高。\n",
    "\n",
    "所以，我们构建一个函数来接收检索到的 10 个文档，并使用跨编码器模型为每个文档提供精确的相关性得分。然后，我们只保留前 3 个，正如我们在 config 中定义的一样。\n",
    "\n",
    "首先，让我们初始化我们的交叉编码器模型。我们将使用 sentence-transformers 库中一个小型但高效的模型，正如我们在配置中指定的一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0028037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CrossEncoder reranker...\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder  # The library for using cross-encoder models\n",
    "\n",
    "print(\"Initializing CrossEncoder reranker...\")\n",
    "# Initialize the CrossEncoder model using the name from our central config dictionary.\n",
    "# The library will automatically download the model from the Hugging Face Hub if it's not cached.\n",
    "reranker = CrossEncoder(config[\"reranker_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892124cd",
   "metadata": {},
   "source": [
    "我们将预先训练好的重排序模型加载到内存中，这只需执行一次。我们选择的模型 `ms-marco-MiniLM-L-6-v2` 非常适合这项任务，因为它在速度和准确率之间实现了良好的平衡。\n",
    "\n",
    "现在我们可以创建执行重新排序的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "802f9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_documents_function(query: str, documents: List[Document]) -> List[Document]:\n",
    "    # If we have no documents to rerank, return an empty list immediately.\n",
    "    if not documents:\n",
    "        return []\n",
    "\n",
    "    # Create the pairs of [query, document_content] that the cross-encoder needs.\n",
    "    pairs = [(query, doc.page_content) for doc in documents]\n",
    "\n",
    "    # Use the reranker to predict a relevance score for each pair. This returns a list of scores.\n",
    "    scores = reranker.predict(pairs)\n",
    "\n",
    "    # Combine the original documents with their new scores.\n",
    "    doc_scores = list(zip(documents, scores))\n",
    "\n",
    "    # Sort the list of (document, score) tuples in descending order based on the score.\n",
    "    doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extract just the Document objects from the top N sorted results.\n",
    "    # The number of documents to keep is controlled by 'top_n_rerank' in our config.\n",
    "    reranked_docs = [doc for doc, score in doc_scores[:config[\"top_n_rerank\"]]]\n",
    "\n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88a9ee",
   "metadata": {},
   "source": [
    "这个函数（`rerank_documents_function`）是我们精确度阶段的主要部分。它接收 `query` 和来自召回阶段的 10 个 `documents` 列表。最重要的步骤是 `reranker.predict(pairs)`。\n",
    "\n",
    "在这里，模型不会创建嵌入，而是对查询与每个文档内容进行全面比较，并为每个文档生成相关性分数。获得分数后，我们只需对文档进行排序并对列表进行切片以仅保留前 3 个。此功能的输出将是一个简短、干净且高度相关的文档列表，这是我们下游代理的完美上下文。\n",
    "\n",
    "这种漏斗式方法，从高召回率的第一阶段过渡到高精度的第二阶段，是生产级 RAG 系统的一个组成部分。它确保我们获得最佳证据，同时最大限度地降低噪音和成本。\n",
    "\n",
    "### 7.4 使用上下文提炼进行合成\n",
    "\n",
    "现在，我们的检索漏斗运行得非常好。我们先进行大范围搜索，找到了 10 篇可能相关的文档。然后，我们的高精度重排序器筛选出最相关的前 3 篇文档，但在将这些信息交给主要推理代理之前，我们仍然可以做最后的改进。目前，我们有三个独立的文本块。\n",
    "\n",
    "虽然它们都相关，但可能包含冗余信息或重叠句子。将它们呈现为三个不同的区块，对于 LLM 来说，处理起来仍然有些笨拙。\n",
    "\n",
    "我们检索漏斗的最后一个阶段是语境提炼。目标很简单：提取最相关的三个文档块，并将它们提炼成一个简洁明了的段落。这将消除所有冗余信息，并为我们的下游代理提供完美合成的证据。\n",
    "\n",
    "这个提炼步骤充当了最后的压缩层。它确保输入到我们更昂贵的推理代理中的上下文尽可能密集且信息丰富，从而最大化信号并最小化噪声。为此，我们将创建另一个小型的专用代理，我们将其称为 Distiller Agent。\n",
    "\n",
    "首先，我们需要设计引导其行为的提示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de8e0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompt for our distiller agent, instructing it to synthesize and be concise\n",
    "distiller_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a helpful assistant. Your task is to synthesize the \n",
    "            following retrieved document snippets into a single, concise paragraph.\n",
    "            The goal is to provide a clear and coherent context that directly \n",
    "            answers the question: '{question}'. Focus on removing redundant \n",
    "            information and organizing the content logically. \n",
    "            Answer only with the synthesized context.\"\"\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\", \n",
    "            \"Retrieved Documents:\\n{context}\"\n",
    "        )  # The content of our top 3 reranked documents will be passed here\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f165ee4",
   "metadata": {},
   "source": [
    "我们给这个代理一个非常专注的任务，告诉它：\"这里有一些文本片段。你唯一的任务就是把它们合并成一个连贯的段落来回答这个特定的问题。\"\"只用合成的上下文来回答\" 这个指令很重要，它可以防止代理添加任何对话废话或试图回答问题本身。它只是一个纯粹的文本处理工具。\n",
    "\n",
    "现在，我们可以组装简单的 `distiller_agent`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21df05f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Distiller Agent created.\n"
     ]
    }
   ],
   "source": [
    "# Create the agent by piping our prompt to the reasoning LLM and a string output parser\n",
    "distiller_agent = distiller_prompt | reasoning_llm | StrOutputParser()\n",
    "print(\"Contextual Distiller Agent created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a01b853",
   "metadata": {},
   "source": [
    "这是另一个简单的 LCEL 链。我们获取 `distiller_prompt`，将其通过管道传输到强大的 `reasoning_llm` 进行合成，然后使用 `StrOutputParser` 获取最终的、干净的文本段落。\n",
    "\n",
    "创建好这个 `distiller_agent` 后，我们的多阶段检索漏斗就完成了。在我们的主代理循环中，每个研究步骤的流程如下：\n",
    "\n",
    "- **主管**：选择一种检索策略（vector、keyword 或 hybrid）。\n",
    "- **召回阶段**：执行选定的策略以获取前 10 个文档。\n",
    "- **精确阶段**：使用 `rerank_documents_function` 获取前 3 个文档。\n",
    "- **蒸馏阶段**：使用 `distiller_agent` 将前 3 个文档压缩成一个干净的段落。\n",
    "\n",
    "这个多阶段流程确保我们的 agent 所处理的证据具有最高的质量。下一步是赋予我们的 agent 超越其内部知识的能力，并搜索网络。\n",
    "\n",
    "## 八、利用网络搜索增强知识\n",
    "\n",
    "现在，我们的检索漏斗现在非常强大，但它有一个巨大的盲点，它只能看到 2023 年的 10-K 文件的内容。为了解决我们的挑战查询，我们的 agent 需要查找有关 AMD AI 芯片战略的最新消息（提交文件后，从 2024 年开始）。这些信息在我们的静态知识库中根本不存在。\n",
    "\n",
    "要真正打造一个\"深度思考\"的智能体，它需要能够认识到自身知识的局限性，并在其他地方寻找答案。我们需要给它一扇通往外部世界的窗户。\n",
    "\n",
    "在这一步，我们用一个新工具——Web Search——来增强我们 agent 的功能。这将使我们的系统从一个针对特定文档的问答机器人转变为一个真正的多源研究助手。\n",
    "\n",
    "为此，我们将使用 Tavily Search API。这是一个专为 LLM 打造的搜索引擎，提供简洁、无广告且相关的搜索结果，非常适合 RAG 流程。它还能与 LangChain 无缝集成。\n",
    "\n",
    "所以，我们需要做的第一件事就是初始化 Tavily 搜索工具本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a667ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "# Initialize the Tavily search tool.\n",
    "# k=3: This parameter instructs the tool to return the top 3 most relevant search results for a given query.\n",
    "web_search_tool = TavilySearch(max_results=3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d86f89c",
   "metadata": {},
   "source": [
    "创建一个 Tavily 搜索工具的实例，供我们的 agent 调用。`k=3` 参数是一个很好的起点，它提供了一些高质量的来源，而不会让代理承受过多的信息。\n",
    "\n",
    "现在，原始的 API 响应并非我们真正需要的。我们的下游组件，重新排序器和提取器，都设计为与特定的数据结构协同工作：一个 LangChain Document 对象列表。为了确保无缝集成，我们需要创建一个简单的包装函数。该函数将接受查询，调用 Tavily 工具，然后将原始结果格式化为标准的 Document 结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "469e02fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search_function(query: str) -> List[Document]:\n",
    "    # 调用 Tavily 工具\n",
    "    resp: dict = web_search_tool.invoke({\"query\": query})\n",
    "    # resp 是一个 dict，我们从中取出 \"results\" 列表\n",
    "    results = resp.get(\"results\", [])\n",
    "\n",
    "    docs: List[Document] = []\n",
    "    for item in results:\n",
    "        item: dict\n",
    "        # Tavily 返回里通常有 content / url / title 等字段\n",
    "        content = item.get(\"content\") or item.get(\"raw_content\") or \"\"\n",
    "        if not content:\n",
    "            continue\n",
    "        url = item.get(\"url\", \"\")\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=content,\n",
    "                metadata={\"source\": url} if url else {}\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd40626",
   "metadata": {},
   "source": [
    "这个 `web_search_function` 充当着一个至关重要的适配器，它调用 `web_search_tool.invoke` 返回一个字典列表，每个字典都包含诸如 \"content\" 和 \"url\" 之类的键。\n",
    "\n",
    "然后，使用列表推导循环遍历这些结果并将它们整齐地重新打包到我们的管道所需的 Document 对象中。\n",
    "\n",
    "`page_content` 获取主要文本，重要的是，我们将 `url` 存储在 `metadata` 中，这确保了当我们的代理生成最终答案时，它可以正确引用其网络来源。\n",
    "\n",
    "这样，我们的外部知识源看起来和感觉起来与我们的内部知识源完全一样，从而允许我们对两者使用相同的处理管道。\n",
    "\n",
    "函数准备就绪后，我们来快速测试一下，确保它能按预期工作。我们将使用与主要挑战第二部分相关的查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7207b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Web Search Tool ---\n",
      "Found 3 results for query: 'AMD AI chip strategy 2024'\n",
      "Top result snippet: At the Computex technology trade show in Taipei, AMD's CEO Lisa Su introduced the MI325X accelerator, expected to be available in the fourth quarter of 2024. This move signifies AMD's aggressive push to compete with Nvidia {{ m-tag option=\"price\" tic...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Testing Web Search Tool ---\")\n",
    "test_query_web = \"AMD AI chip strategy 2024\"\n",
    "test_results_web = web_search_function(test_query_web)\n",
    "print(f\"Found {len(test_results_web)} results for query: '{test_query_web}'\")\n",
    "if test_results_web:\n",
    "    print(f\"Top result snippet: {test_results_web[0].page_content[:250]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a2601",
   "metadata": {},
   "source": [
    "输出结果证实了我们的工具运行正常。它找到了 3 个与我们的查询相关的网页。顶部结果中的片段正是我们的代理所缺少的那种最新的外部信息。其中提到了 AMD\"Instinct MI300X\"及其与 NVIDIA \"H100\" 的竞争，这正是解决我们问题后半部分所需的证据。\n",
    "\n",
    "我们的智能体现在拥有了一扇通往外部世界的窗户，它的规划器可以智能地决定何时打开这扇窗户。最后一个难题是赋予智能体反思其发现的能力，并决定其研究何时完成。\n",
    "\n",
    "## 九、自我批评和控制流政策\n",
    "\n",
    "到目前为止，我们已经构建了一台强大的研究机器。我们的 agent 可以制定计划，选择合适的工具，并执行复杂的检索漏斗。但它缺少一个关键要素：思考自身进展的能力。一个盲目地、一步步遵循计划的智能体并非真正的智能。它需要一种自我批评的机制。\n",
    "\n",
    "这就是我们构建 agent 自主认知核心的地方。在每个研究步骤之后，我们的 agent 都会暂停并进行反思。它会查看刚刚发现的新信息，并将其与已知的信息进行比较，然后做出战略决策：我的研究完成了吗？还是我需要继续？\n",
    "\n",
    "这种自我批评循环使我们的系统从脚本化的工作流程提升为自主的代理。正是这种机制让它能够判断何时收集到足够的证据，从而自信地回答用户的问题。\n",
    "\n",
    "我们将使用两个新的专门代理来实现这一点：\n",
    "\n",
    "- **Reflection Agent**：该代理将从已完成的步骤中提取上下文，并创建一个简洁的单句摘要，该摘要随后会被添加到我们代理的\"研究历史\"中。\n",
    "- **Policy Agent**：这是战略大师。经过反思，它将根据原计划审查整个研究历史，并做出一个关键决定：`CONTINUE_PLAN` 还是 `FINISH`。\n",
    "\n",
    "### 9.1 更新并反映累积研究历史\n",
    "\n",
    "当我们的代理完成一个研究步骤（例如，检索并提取有关 NVIDIA 风险的信息）后，我们不想就此打住。我们需要将这些新知识整合到代理的记忆中。\n",
    "\n",
    "我们将构建一个反射代理，它将从当前步骤中提取丰富、精炼的上下文，并将其总结成一个单一、真实的句子。然后，这个总结会被添加到 RAGState 中的 `past_steps` 列表中。\n",
    "\n",
    "首先，让我们为该代理创建提示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12bbf829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompt for our reflection agent, instructing it to be concise and factual\n",
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a research assistant. Based on the retrieved context \n",
    "            for the current sub-question, write a concise, one-sentence summary \n",
    "            of the key findings. This summary will be added to our research history. \n",
    "            Be factual and to the point.\"\"\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Current sub-question: {sub_question}\\n\\nDistilled context:\\n{context}\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4458c26",
   "metadata": {},
   "source": [
    "我们让这个智能体像一个勤奋的研究助理一样工作。它的任务不是发挥创造力，而是做好笔记。它会阅读 `context` 并撰写 `summary`。现在我们可以组装智能体本身了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89e84445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reflection Agent created.\n"
     ]
    }
   ],
   "source": [
    "# Create the agent by piping our prompt to the reasoning LLM and a string output parser\n",
    "reflection_agent = reflection_prompt | reasoning_llm | StrOutputParser()\n",
    "print(\"Reflection Agent created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad0f33e",
   "metadata": {},
   "source": [
    "这个 `reflection_agent` 是我们认知循环的一部分。通过创建这些简洁的摘要，它构建了一个清晰易读的研究历史记录。这些历史记录将成为我们下一个也是最重要的代理的输入：决定何时停止的代理。\n",
    "\n",
    "### 9.2 构建控制流策略代理\n",
    "\n",
    "这是我们自主 agent 的大脑。在 `reflection_agent` 更新了研究历史记录后，策略代理便开始发挥作用。它充当整个操作的监督者。\n",
    "\n",
    "它的工作是查看代理所知道的一切——原始问题、初步计划以及已完成步骤的完整摘要历史记录，并做出高层战略决策。\n",
    "\n",
    "我们将首先使用 Pydantic 模型定义其决策的结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79777c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision(BaseModel):\n",
    "    # The decision must be one of these two actions.\n",
    "    next_action: Literal[\"CONTINUE_PLAN\", \"FINISH\"]\n",
    "    # The agent must justify its decision.\n",
    "    justification: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574adeb2",
   "metadata": {},
   "source": [
    "这个 Decision 类强制我们的策略代理做出明确的二元选择，并解释其推理，这使得它的行为透明且易于调试。\n",
    "\n",
    "接下来，我们设计指导其决策过程的提示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "500494d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompt for our policy agent, instructing it to act as a master strategist\n",
    "policy_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a master strategist. Your role is to analyze the research progress \n",
    "            and decide the next action. You have the original question, the initial plan, \n",
    "            and a log of completed steps with their summaries.\n",
    "            - If the collected information in the Research History is sufficient to \n",
    "            comprehensively answer the Original Question, decide to FINISH.\n",
    "            - Otherwise, if the plan is not yet complete, decide to CONTINUE_PLAN.\"\"\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Original Question: {question}\\n\\nInitial Plan:\\n{plan}\\n\\nResearch History (Completed Steps):\\n{history}\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7970b1",
   "metadata": {},
   "source": [
    "我们要求 LLM 进行元分析，它不是回答问题本身，而是对研究过程的状态进行推理。它将现有信息（`history`）与所需信息（`plan` 和 `question`）进行比较，并做出判断。\n",
    "\n",
    "现在，我们可以组装 `policy_agent`。\n",
    "\n",
    "为了正确测试我们的 `policy_agent`，我们模拟了代理生命周期中的两个不同时刻。在第一个测试中，我们为其提供仅包含步骤 1 摘要的历史记录。在第二个测试中，我们为其提供步骤 1 和步骤 2 的摘要。\n",
    "\n",
    "让我们来研究一下它在每种情况下做出的决定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "815abd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Agent created.\n",
      "\n",
      "--- Testing Policy Agent (Incomplete State) ---\n",
      "Decision: CONTINUE_PLAN, Justification: The research is not yet complete. We have identified that NVIDIA's 10-K filing mentions intense competition and rapid technological change as key risks. However, we still need to gather information on AMD's AI chip strategy in 2024 to understand how it might impact NVIDIA's competitive position. This will allow us to directly address the original question by comparing AMD's strategy with NVIDIA's stated risks.\n",
      "\n",
      "--- Testing Policy Agent (Complete State) ---\n",
      "Decision: CONTINUE_PLAN, Justification: The research so far has identified NVIDIA's general competitive risks and AMD's recent strategic move in the AI chip market. However, to comprehensively answer the original question, we need to explicitly connect AMD's 2024 AI chip strategy to the specific competitive risks mentioned by NVIDIA. This requires further analysis to determine how AMD's MI300X launch and its adoption by major cloud providers directly address or exacerbate NVIDIA's stated risks. Therefore, the next step in the plan, which involves analyzing the impact of AMD's strategy on NVIDIA's competitive risks, should be completed.\n"
     ]
    }
   ],
   "source": [
    "# Create the agent by piping our prompt to the reasoning LLM \n",
    "# and structuring its output with our Decision class\n",
    "policy_agent = (\n",
    "    policy_prompt \n",
    "    | reasoning_llm.with_structured_output(Decision)\n",
    ")\n",
    "print(\"Policy Agent created.\")\n",
    "\n",
    "# Now, let's test the policy agent with two different states of our research process\n",
    "print(\"\\n--- Testing Policy Agent (Incomplete State) ---\")\n",
    "\n",
    "# First, a state where only Step 1 is complete.\n",
    "plan_str = json.dumps(\n",
    "    [s.model_dump() for s in test_plan.steps]\n",
    ")\n",
    "incomplete_history = (\n",
    "    \"Step 1 Summary: NVIDIA's 10-K states that the semiconductor industry \"\n",
    "    \"is intensely competitive and subject to rapid technological change.\"\n",
    ")\n",
    "\n",
    "decision1 = policy_agent.invoke(\n",
    "    {\n",
    "        \"question\": complex_query_adv,\n",
    "        \"plan\": plan_str, \n",
    "        \"history\": incomplete_history\n",
    "    }\n",
    ")\n",
    "print(\n",
    "    f\"Decision: {decision1.next_action}, \"\n",
    "    f\"Justification: {decision1.justification}\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- Testing Policy Agent (Complete State) ---\")\n",
    "\n",
    "# Second, a state where both Step 1 and Step 2 are complete.\n",
    "complete_history = (\n",
    "    incomplete_history \n",
    "    + \"\\nStep 2 Summary: In 2024, AMD launched its MI300X accelerator \"\n",
    "    \"to directly compete with NVIDIA in the AI chip market, \"\n",
    "    \"gaining adoption from major cloud providers.\"\n",
    ")\n",
    "\n",
    "decision2 = policy_agent.invoke(\n",
    "    {\n",
    "        \"question\": complex_query_adv,\n",
    "        \"plan\": plan_str,\n",
    "        \"history\": complete_history\n",
    "    }\n",
    ")\n",
    "print(\n",
    "    f\"Decision: {decision2.next_action}, \"\n",
    "    f\"Justification: {decision2.justification}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e3fd8",
   "metadata": {},
   "source": [
    "让我们了解一下输出……\n",
    "\n",
    "在不完整状态下，agent 正确识别出缺少有关 AMD 策略的信息。它查看了计划，发现下一步是使用网络搜索，并正确决定执行 `CONTINUE_PLAN`。\n",
    "\n",
    "在完整阶段，在获得网络搜索的摘要后，它再次分析了其历史记录。这一次，它意识到自己已经掌握了 NVIDIA 风险和 AMD 战略的所有信息。它正确地决定，研究已经完成，是时候 `FINISH` 了。\n",
    "\n",
    "通过这个 `policy_agent`，我们构建了自主系统的大脑。最后一步是使用 LangGraph 将所有这些组件连接成一个完整的、可执行的工作流。\n",
    "\n",
    "## 十、定义图形节点\n",
    "\n",
    "我们已经设计了所有这些酷炫的专业 agent。现在是时候将它们转化为我们工作流程的实际构建块了。在 LangGraph 中，这些构建块被称为节点。节点只是一个执行特定任务的 Python 函数。它以代理的当前内存（`RAGState`）作为输入，执行其任务，然后返回一个包含该内存任何更新的字典。\n",
    "\n",
    "我们将为 agent 需要采取的每个主要步骤创建一个节点。\n",
    "\n",
    "首先，我们需要一个简单的辅助函数。由于我们的 agent 经常需要查看研究历史记录，因此我们需要一种简洁的方法将 `past_steps` 列表格式化为可读的字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "19ab9c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_past_context_str(past_steps: List[\"PastStep\"]) -> str:\n",
    "    # This takes the list of PastStep dictionaries and joins them into a single string.\n",
    "    # Each step is clearly labeled for the LLM to understand the context.\n",
    "    return \"\\n\\n\".join([\n",
    "        f\"Step {s['step_index']}: {s['sub_question']}\\nSummary: {s['summary']}\"\n",
    "        for s in past_steps\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8bf30",
   "metadata": {},
   "source": [
    "我们创建一个实用程序，它将在我们的几个节点内使用，为我们的提示提供历史背景。\n",
    "\n",
    "现在来看看我们的第一个真实节点：`plan_node`。这是我们 agent 推理的起点。它唯一的任务就是调用我们的 `planner_agent` 并填充 RAGState 中的 `plan` 字段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0bccc2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 1: The Planner\n",
    "def plan_node(state: \"RAGState\") -> Dict:\n",
    "    console.print(\"--- 🧠: Plan Node ---\")\n",
    "\n",
    "    # 如果已经有 plan，说明不是第一次了，直接复用，不要重置索引和历史\n",
    "    if \"plan\" in state and state[\"plan\"] is not None:\n",
    "        console.print(\n",
    "            f\"  Reusing existing plan. current_step_index = {state.get('current_step_index', 0)}\"\n",
    "        )\n",
    "        # 返回空 dict，表示不修改 state\n",
    "        return {}\n",
    "\n",
    "    # 第一次进入，真正去调用 planner 生成计划\n",
    "    console.print(\"  No existing plan. Generating new plan...\")\n",
    "    plan = planner_agent.invoke({\"question\": state[\"original_question\"]})\n",
    "    rprint(plan)\n",
    "\n",
    "    # 初始化 current_step_index 和 past_steps\n",
    "    return {\"plan\": plan, \"current_step_index\": 0, \"past_steps\": []}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0434b7f",
   "metadata": {},
   "source": [
    "此节点启动所有操作。它从状态中获取 `original_question`，获取 plan，然后将 `current_step_index` 初始化为 0（从第一步开始），并清除 `past_steps` 历史记录，以便进行新的运行。\n",
    "\n",
    "接下来，我们需要实际查找信息的节点。由于我们的规划器可以在两种工具之间进行选择，因此我们需要两个独立的检索节点。让我们从用于搜索内部 10K 文档的 `retrieval_node` 开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "68170f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 2a: Retrieval from the 10-K document\n",
    "def retrieval_node(state: \"RAGState\") -> Dict:\n",
    "    # First, get the details for the current step in the plan.\n",
    "    current_step_index = state[\"current_step_index\"]\n",
    "    current_step = state[\"plan\"].steps[current_step_index]\n",
    "    console.print(\n",
    "        f\"--- 🔍: Retrieving from 10-K (Step {current_step_index + 1}: {current_step.sub_question}) ---\")\n",
    "\n",
    "    # Use our query rewriter to optimize the sub-question for search.\n",
    "    past_context = get_past_context_str(state[\"past_steps\"])\n",
    "    rewritten_query = query_rewriter_agent.invoke({\n",
    "        \"sub_question\": current_step.sub_question,\n",
    "        \"keywords\": current_step.keywords,\n",
    "        \"past_context\": past_context\n",
    "    })\n",
    "    console.print(f\"  Rewritten Query: {rewritten_query}\")\n",
    "\n",
    "    # Get the supervisor's decision on which retrieval strategy is best.\n",
    "    retrieval_decision = retrieval_supervisor_agent.invoke({\"sub_question\": rewritten_query})\n",
    "    console.print(\n",
    "        f\"\"\"  Supervisor Decision: Use `{retrieval_decision.strategy}`. \n",
    "        Justification: {retrieval_decision.justification}\"\"\"\n",
    "    )\n",
    "\n",
    "    # Based on the decision, execute the correct retrieval function.\n",
    "    if retrieval_decision.strategy == \"vector_search\":\n",
    "        retrieved_docs = vector_search_only(\n",
    "            rewritten_query,\n",
    "            section_filter=current_step.document_section,\n",
    "            k=config[\"top_k_retrieval\"]\n",
    "        )\n",
    "    elif retrieval_decision.strategy == \"keyword_search\":\n",
    "        retrieved_docs = bm25_search_only(\n",
    "            rewritten_query,\n",
    "            k=config[\"top_k_retrieval\"]\n",
    "        )\n",
    "    else:  # hybrid_search\n",
    "        retrieved_docs = hybrid_search(\n",
    "            rewritten_query,\n",
    "            section_filter=current_step.document_section,\n",
    "            k=config[\"top_k_retrieval\"]\n",
    "        )\n",
    "\n",
    "    # Return the retrieved documents to be added to the state.\n",
    "    return {\"retrieved_docs\": retrieved_docs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69438e3b",
   "metadata": {},
   "source": [
    "这个节点可以执行大量智能工作，不仅仅是一个简单的检索器。它编排了一个微型管道：它重写查询，向主管请求最佳策略，然后执行该策略。\n",
    "\n",
    "现在，我们需要另一个工具的对应节点：网络搜索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "61f7bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 2b: Retrieval from the Web\n",
    "def web_search_node(state: \"RAGState\") -> Dict:\n",
    "    # Get the details for the current step.\n",
    "    current_step_index = state[\"current_step_index\"]\n",
    "    current_step = state[\"plan\"].steps[current_step_index]\n",
    "    console.print(\n",
    "        f\"--- 🌐: Searching Web (Step {current_step_index + 1}: {current_step.sub_question}) ---\")\n",
    "\n",
    "    # Rewrite the sub-question for a web search engine.\n",
    "    past_context = get_past_context_str(state[\"past_steps\"])\n",
    "    rewritten_query = query_rewriter_agent.invoke({\n",
    "        \"sub_question\": current_step.sub_question,\n",
    "        \"keywords\": current_step.keywords,\n",
    "        \"past_context\": past_context\n",
    "    })\n",
    "    console.print(f\"  Rewritten Query: {rewritten_query}\")\n",
    "\n",
    "    # Call our web search function.\n",
    "    retrieved_docs = web_search_function(rewritten_query)\n",
    "\n",
    "    # Return the results.\n",
    "    return {\"retrieved_docs\": retrieved_docs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4114d",
   "metadata": {},
   "source": [
    "这个 `web_search_node` 更简单，因为它不需要 supervisor，只有一种搜索网络的方法。但它仍然使用我们强大的查询重写器来确保搜索尽可能高效。\n",
    "\n",
    "检索文档（无论来自哪个来源）后，我们需要运行精度和综合漏斗。我们将为每个阶段创建一个节点。首先是 `rerank_node`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "be2de222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 3: The Reranker\n",
    "def rerank_node(state: RAGState) -> Dict:\n",
    "    console.print(\"--- 🎯: Reranking Documents ---\")\n",
    "    # Get the current step's details.\n",
    "    current_step_index = state[\"current_step_index\"]\n",
    "    current_step = state[\"plan\"].steps[current_step_index]\n",
    "    # Call our reranking function on the documents we just retrieved.\n",
    "    reranked_docs = rerank_documents_function(\n",
    "        current_step.sub_question,\n",
    "        state[\"retrieved_docs\"]\n",
    "    )\n",
    "    console.print(f\"  Reranked to top {len(reranked_docs)} documents.\")\n",
    "    # Update the state with the high-precision documents.\n",
    "    return {\"reranked_docs\": reranked_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b0f40",
   "metadata": {},
   "source": [
    "该节点获取 `retrieved_docs`（我们对 10 个文档的广泛回忆）并使用交叉编码器将它们过滤到前 3 个，并将结果放在 `reranked_docs` 中。\n",
    "\n",
    "接下来，`compression_node` 将获取前 3 个文档并对其进行提炼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c3bc7982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 4: The Compressor / Distiller\n",
    "def compression_node(state: RAGState) -> Dict:\n",
    "    console.print(\"--- ✂️: Distilling Context ---\")\n",
    "    # Get the current step's details.\n",
    "    current_step_index = state[\"current_step_index\"]\n",
    "    current_step = state[\"plan\"].steps[current_step_index]\n",
    "    # Format the top 3 documents into a single string.\n",
    "    context = format_docs(state[\"reranked_docs\"])\n",
    "    # Call our distiller agent to synthesize them into one paragraph.\n",
    "    synthesized_context = distiller_agent.invoke({\n",
    "        \"question\": current_step.sub_question,\n",
    "        \"context\": context,\n",
    "    })\n",
    "    console.print(f\"  Distilled Context Snippet: {synthesized_context[:200]}...\")\n",
    "    # Update the state with the final, clean context.\n",
    "    return {\"synthesized_context\": synthesized_context}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d271b",
   "metadata": {},
   "source": [
    "此节点是我们检索漏斗的最后一步。它获取 `reranked_docs` 并生成一个干净的 `synthesized_context` 段落。\n",
    "\n",
    "现在我们有了证据，我们需要反思它并更新我们的研究历史。这就是 `reflection_node` 的工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c1daa7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 5: The Reflection / Update Step\n",
    "def reflection_node(state: RAGState) -> Dict:\n",
    "    console.print(\"--- 🤔: Reflecting on Findings ---\")\n",
    "    # Get the current step's details.\n",
    "    current_step_index = state[\"current_step_index\"]\n",
    "    current_step = state[\"plan\"].steps[current_step_index]\n",
    "    # Call our reflection agent to summarize the findings.\n",
    "    summary = reflection_agent.invoke({\n",
    "        \"sub_question\": current_step.sub_question,\n",
    "        \"context\": state['synthesized_context'],\n",
    "    })\n",
    "    console.print(f\"  Summary: {summary}\")\n",
    "\n",
    "    # Create a new PastStep dictionary with all the results from this step.\n",
    "    new_past_step = {\n",
    "        \"step_index\": current_step_index + 1,\n",
    "        \"sub_question\": current_step.sub_question,\n",
    "        \"retrieved_docs\": state['reranked_docs'],  # We save the reranked docs for final citation\n",
    "        \"summary\": summary\n",
    "    }\n",
    "    # Append the new step to our history and increment the step index to move to the next step.\n",
    "    return {\n",
    "        \"past_steps\": state[\"past_steps\"] + [new_past_step],\n",
    "        \"current_step_index\": current_step_index + 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b644c89",
   "metadata": {},
   "source": [
    "这个节点是我们 agent 的簿记员。它调用 `reflection_agent` 创建摘要，然后将当前研究周期的所有结果整齐地打包到 `new_past_step` 对象中。然后，它将这个对象添加到 `past_steps` 列表中，并增加 `current_step_index` 值，使代理为下一个循环做好准备。\n",
    "\n",
    "最后，当研究完成时，我们需要最后一个节点来生成最终答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "990355d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 6: The Final Answer Generator\n",
    "def final_answer_node(state: RAGState) -> Dict:\n",
    "    console.print(\"--- ✅: Generating Final Answer with Citations ---\")\n",
    "    # First, we need to gather all the evidence we've collected from ALL past steps.\n",
    "    final_context = \"\"\n",
    "    for i, step in enumerate(state['past_steps']):\n",
    "        final_context += f\"\\n--- Findings from Research Step {i+1} ---\\n\"\n",
    "        # We include the source metadata (section or URL) for each document to enable citations.\n",
    "        for doc in step['retrieved_docs']:\n",
    "            source = doc.metadata.get('section') or doc.metadata.get('source')\n",
    "            final_context += f\"Source: {source}\\nContent: {doc.page_content}\\n\\n\"\n",
    "\n",
    "    # We create a new prompt specifically for generating the final, citable answer.\n",
    "    final_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are an expert financial analyst. Synthesize the research findings \n",
    "                from internal documents and web searches into a comprehensive, \n",
    "                multi-paragraph answer for the user's original question.\n",
    "                Your answer must be grounded in the provided context. \n",
    "                At the end of any sentence that relies on specific information, \n",
    "                you MUST add a citation. \n",
    "                For 10-K documents, use [Source: <section title>]. \n",
    "                For web results, use [Source: <URL>].\"\"\"\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Original Question: {question}\\n\\nResearch History and Context:\\n{context}\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # We create a temporary agent for this final task and invoke it.\n",
    "    final_answer_agent = final_answer_prompt | reasoning_llm | StrOutputParser()\n",
    "    final_answer = final_answer_agent.invoke({\n",
    "        \"question\": state['original_question'],\n",
    "        \"context\": final_context,\n",
    "    })\n",
    "    # Update the state with the final answer.\n",
    "    return {\"final_answer\": final_answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993ac84b",
   "metadata": {},
   "source": [
    "这个 `final_answer_node` 是我们最终的收官之作。它将 `past_steps` 历史记录中每一步的所有高质量、重新排序的文档整合成一个庞大的上下文。然后，它使用专用的提示来指示我们强大的 `reasoning_llm` 将这些信息综合成一个包含引文的、全面的、多段落的答案，从而让我们的研究过程圆满结束。\n",
    "\n",
    "定义好所有节点后，我们现在有了 agent 的所有构建块。下一步是定义连接它们并控制图的流程的\"线路\"。\n",
    "\n",
    "## 十一、定义条件边\n",
    "\n",
    "至此，我们已经构建好了所有节点。我们有一个规划器、检索器、一个重排序器、一个提取器和一个反射器。可以把它们想象成一个房间里的一群专家。现在我们需要定义对话规则。谁在什么时候发言？我们如何决定下一步做什么？\n",
    "\n",
    "这就是 LangGraph 中边的作用。简单的边很简单，\"在节点 A 之后，总是指向节点 B\"。但真正的智能来自条件边。\n",
    "\n",
    "条件边是一种函数，它查看代理的当前内存（`RAGState`）并做出决策，根据情况将工作流程路由到不同的路径。\n",
    "\n",
    "我们的 agent 需要两个关键的决策功能：\n",
    "\n",
    "- **工具路由器（`route_by_tool`）**：制定计划后，此功能将查看计划的当前步骤并决定是否将工作流发送到 `retrieve_10k` 节点或 `retrieve_web` 节点。\n",
    "- **主控制循环（`should_continue_node`）**：这是最重要的一个。在每个研究步骤完成并反思之后，此函数将调用我们的 `policy_agent` 来决定是继续执行计划的下一步，还是完成研究并生成最终答案。\n",
    "\n",
    "首先，让我们构建简单的工具路由器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "978fb246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional Edge 1: The Tool Router\n",
    "def route_by_tool(state: RAGState) -> str:\n",
    "    # Get the index of the current step we are on.\n",
    "    current_step_index = state[\"current_step_index\"]\n",
    "    \n",
    "    # Get the full details of the current step from the plan.\n",
    "    current_step = state[\"plan\"].steps[current_step_index]\n",
    "    \n",
    "    # Return the name of the tool specified for this step.\n",
    "    # LangGraph will use this string to decide which node to go to next.\n",
    "    return current_step.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efbbed2",
   "metadata": {},
   "source": [
    "这个函数非常简单，但至关重要。它就像一个总机，从状态中读取 `current_step_index`，在 `plan` 中找到对应的 Step，并返回其 `tool` 字段的值（可能是 \"search_10k\" 或 \"search_web\"）。当我们连接图时，我们会告诉它使用这个函数的输出来选择下一个节点。\n",
    "\n",
    "现在我们需要创建一个函数来控制代理的主要推理循环。这就是我们的 `policy_agent` 发挥作用的地方。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "533e8cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional Edge 2: The Main Control Loop\n",
    "def should_continue_node(state: RAGState) -> str:\n",
    "    console.print(\"--- 🚦: Evaluating Policy ---\")\n",
    "    \n",
    "    # Get the index of the step we are about to start.\n",
    "    current_step_index = state[\"current_step_index\"]\n",
    "    \n",
    "    # First, check our basic stopping conditions.\n",
    "    \n",
    "    # Condition 1: Have we completed all the steps in the plan?\n",
    "    if current_step_index >= len(state[\"plan\"].steps):\n",
    "        console.print(\"  -> Plan complete. Finishing.\")\n",
    "        return \"finish\"\n",
    "    \n",
    "    # Condition 2: Have we exceeded our safety limit for the number of iterations?\n",
    "    if current_step_index >= config[\"max_reasoning_iterations\"]:\n",
    "        console.print(\"  -> Max iterations reached. Finishing.\")\n",
    "        return \"finish\"\n",
    "    \n",
    "    # A special case: If the last retrieval step failed to find any documents,\n",
    "    # there's no point in reflecting. It's better to just move on to the next step.\n",
    "    if (\n",
    "        state.get(\"reranked_docs\") is not None \n",
    "        and not state[\"reranked_docs\"]\n",
    "    ):\n",
    "        console.print(\n",
    "            \"  -> Retrieval failed for the last step. \"\n",
    "            \"Continuing with next step in plan.\"\n",
    "        )\n",
    "        return \"continue\"\n",
    "    \n",
    "    # If none of the basic conditions are met, it's time to ask our Policy Agent.\n",
    "    # We format the history and plan into strings for the prompt.\n",
    "    history = get_past_context_str(state['past_steps'])\n",
    "    plan_str = json.dumps(\n",
    "        [\n",
    "            s.model_dump() # s.dict() is deprecated\n",
    "            for s in state['plan'].steps\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Invoke the policy agent to get its strategic decision.\n",
    "    decision = policy_agent.invoke(\n",
    "        {\n",
    "            \"question\": state[\"original_question\"],\n",
    "            \"plan\": plan_str, \n",
    "            \"history\": history\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    console.print(\n",
    "        f\"  -> Decision: {decision.next_action} | \"\n",
    "        f\"Justification: {decision.justification}\"\n",
    "    )\n",
    "    \n",
    "    # Based on the agent's decision, return the appropriate signal.\n",
    "    if decision.next_action == \"FINISH\":\n",
    "        return \"finish\"\n",
    "    else:  # CONTINUE_PLAN\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c1c9a",
   "metadata": {},
   "source": [
    "这个 `should_continue_node` 函数是我们代理控制流的认知核心。它在每个 `reflection_node` 之后运行。\n",
    "\n",
    "它首先检查简单的、硬编码的停止条件。计划是否用完了所有步骤？是否达到了 `max_reasoning_iterations` 的安全限制？这些条件可以防止智能体永远运行下去。\n",
    "\n",
    "如果这些检查通过，它就会调用我们强大的 `policy_agent`。它为策略代理提供所需的所有上下文：原始目标（`question`）、完整 `plan` 以及迄今为止已完成工作的 `history`。\n",
    "\n",
    "最后，它获取 `policy_agent` 的结构化输出（`CONTINUE_PLAN` 或 `FINISH`），并返回简单字符串 \"continue\" 或 \"finish\"。LangGraph 将使用此字符串来循环回另一个研究周期或继续执行 `final_answer_node`。\n",
    "\n",
    "现在，我们已经定义了节点（专家）和条件边（对话规则），并且拥有了所需的一切。\n",
    "\n",
    "现在是时候将所有这些部分组装成一个完整的、可运行的 StateGraph 了。\n",
    "\n",
    "## 十二、连接深度思考 RAG 机器\n",
    "\n",
    "我们已经准备好所有单独的组件：我们的节点（工作者）我们的条件边缘（经理）。现在是时候将它们全部连接起来形成一个单一的、有凝聚力的系统了。\n",
    "\n",
    "我们将使用 LangGraph 的 StateGraph 来定义智能体的完整认知架构。在这里，我们规划了 agent 思维过程的蓝图，并精确定义了信息如何从一个步骤流向下一个步骤。\n",
    "\n",
    "我们要做的第一件事是创建一个 StateGraph 实例。我们会告诉它，它将要传递的\"状态\"是我们的 RAGState 字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f26db992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import (\n",
    "    StateGraph, \n",
    "    END\n",
    ")  # Import the main graph components\n",
    "\n",
    "# Instantiate the graph, telling it to use our RAGState TypedDict as its state schema.\n",
    "graph = StateGraph(RAGState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecef6ab",
   "metadata": {},
   "source": [
    "现在我们有了一个空图。下一步是添加我们之前定义的所有节点。`.add_node()` 方法接受两个参数：节点的唯一字符串名称，以及该节点将执行的 Python 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0b702e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x22933d60910>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add all of our Python functions as nodes in the graph\n",
    "graph.add_node(\"plan\", plan_node)                      # The node that creates the initial plan\n",
    "graph.add_node(\"retrieve_10k\", retrieval_node)         # The node for internal document retrieval\n",
    "graph.add_node(\"retrieve_web\", web_search_node)        # The node for external web search\n",
    "graph.add_node(\"rerank\", rerank_node)                  # The node that performs precision reranking\n",
    "graph.add_node(\"compress\", compression_node)           # The node that distills the context\n",
    "graph.add_node(\"reflect\", reflection_node)             # The node that summarizes findings and updates history\n",
    "graph.add_node(\"generate_final_answer\", final_answer_node)  # The node that synthesizes the final answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c29c0",
   "metadata": {},
   "source": [
    "现在所有专家都到齐了。最后也是最关键的一步是定义连接他们的\"线路\"。在这里，我们使用 `.add_edge()` 和 `.add_conditional_edges()` 方法来定义控制流。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "707989ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateGraph constructed successfully.\n"
     ]
    }
   ],
   "source": [
    "# The entry point of our graph is the \"plan\" node. Every run starts here.\n",
    "graph.set_entry_point(\"plan\")\n",
    "\n",
    "# After the \"plan\" node, we use our first conditional edge to decide which tool to use.\n",
    "graph.add_conditional_edges(\n",
    "    \"plan\",           # The source node\n",
    "    route_by_tool,    # The function that makes the decision\n",
    "    {                 # A dictionary mapping the function's output string to the destination node\n",
    "        \"search_10k\": \"retrieve_10k\",\n",
    "        \"search_web\": \"retrieve_web\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# After retrieving from either the 10-K or the web, the flow is linear for a bit.\n",
    "graph.add_edge(\"retrieve_10k\", \"rerank\")  # After internal retrieval, always go to rerank.\n",
    "graph.add_edge(\"retrieve_web\", \"rerank\")  # After web retrieval, also always go to rerank.\n",
    "graph.add_edge(\"rerank\", \"compress\")      # After reranking, always go to compress.\n",
    "graph.add_edge(\"compress\", \"reflect\")     # After compressing, always go to reflect.\n",
    "\n",
    "# After the \"reflect\" node, we hit our main conditional edge, which controls the reasoning loop.\n",
    "graph.add_conditional_edges(\n",
    "    \"reflect\",        # The source node\n",
    "    should_continue_node,  # The function that calls our Policy Agent\n",
    "    {                 # A dictionary mapping the decision to the next step\n",
    "        \"continue\": \"plan\",  # If the decision is \"continue\", we loop back to the \"plan\" node to route the next step.\n",
    "        \"finish\": \"generate_final_answer\",  # If the decision is \"finish\", we proceed to generate the final answer.\n",
    "    },\n",
    ")\n",
    "\n",
    "# The \"generate_final_answer\" node is the last step before the end.\n",
    "graph.add_edge(\"generate_final_answer\", END)  # After generating the answer, the graph concludes.\n",
    "\n",
    "print(\"StateGraph constructed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8d97e4",
   "metadata": {},
   "source": [
    "这是我们的 agent 大脑的蓝图。让我们来追踪一下它的流程：\n",
    "\n",
    "一切总是按 `plan` 开始。然后，`route_by_tool` 条件边充当开关，将流引导至 `retrieve_10k` 或 `retrieve_web`。\n",
    "\n",
    "无论运行哪个检索器，输出总是通过 `rerank` -> `compress` -> `reflect` 管道进行传输。\n",
    "\n",
    "这引出了最重要的部分：`should_continue_node` 条件边。这是我们循环推理的核心。\n",
    "\n",
    "- 如果策略代理说 `CONTINUE_PLAN`，边缘节点会将工作流一路发送回 `plan` 节点。我们返回到 `plan`（而不是直接发送到下一个检索器），以便 `route_by_tool` 能够正确地路由计划中的下一步。\n",
    "- 如果策略代理说 `FINISH`，边缘将中断循环并将工作流发送到 `generate_final_answer` 节点。\n",
    "\n",
    "最后，在生成答案后，图表在 `END` 处终止。\n",
    "\n",
    "我们已经成功定义了深度思考 agent 的完整、复杂且循环的架构。剩下要做的就是将这个蓝图编译成一个可运行的应用程序，并将其可视化，以查看我们构建的内容。\n",
    "\n",
    "## 十三、编译和可视化迭代工作流\n",
    "\n",
    "当我们的图完全连接好后，组装过程的最后一步就是编译它。`.compile()` 方法将我们对节点和边的抽象定义转换为具体的可执行应用程序。\n",
    "\n",
    "然后，我们可以使用内置的 LangGraph 实用程序生成图的示意图。可视化工作流程对于理解和调试复杂的代理系统非常有帮助。它将我们的代码转换为直观的流程图，清晰地显示代理可能的推理路径。\n",
    "\n",
    "所以，基本上，我们正在将我们的蓝图变成一台真正的机器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a06cac70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph compiled successfully.\n",
      "pygraphviz PNG failed: Install pygraphviz to draw graphs: `pip install pygraphviz`.\n",
      "Fallback to Mermaid diagram...\n",
      "Graph visualization rendered as Mermaid.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```mermaid\n",
       "---\n",
       "config:\n",
       "  flowchart:\n",
       "    curve: linear\n",
       "---\n",
       "graph TD;\n",
       "\t__start__([<p>__start__</p>]):::first\n",
       "\tplan(plan)\n",
       "\tretrieve_10k(retrieve_10k)\n",
       "\tretrieve_web(retrieve_web)\n",
       "\trerank(rerank)\n",
       "\tcompress(compress)\n",
       "\treflect(reflect)\n",
       "\tgenerate_final_answer(generate_final_answer)\n",
       "\t__end__([<p>__end__</p>]):::last\n",
       "\t__start__ --> plan;\n",
       "\tcompress --> reflect;\n",
       "\tplan -. &nbsp;search_10k&nbsp; .-> retrieve_10k;\n",
       "\tplan -. &nbsp;search_web&nbsp; .-> retrieve_web;\n",
       "\treflect -. &nbsp;finish&nbsp; .-> generate_final_answer;\n",
       "\treflect -. &nbsp;continue&nbsp; .-> plan;\n",
       "\trerank --> compress;\n",
       "\tretrieve_10k --> rerank;\n",
       "\tretrieve_web --> rerank;\n",
       "\tgenerate_final_answer --> __end__;\n",
       "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
       "\tclassDef first fill-opacity:0\n",
       "\tclassDef last fill:#bfb6fc\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The .compile() method takes our graph definition and creates a runnable object.\n",
    "deep_thinking_rag_graph = graph.compile()\n",
    "print(\"Graph compiled successfully.\")\n",
    "\n",
    "# Now, let's visualize the architecture we've built.\n",
    "try:\n",
    "    from IPython.display import Image, display, Markdown\n",
    "\n",
    "    g = deep_thinking_rag_graph.get_graph()\n",
    "\n",
    "    # ① 优先尝试用 pygraphviz 画 PNG（如果以后你装好了 pygraphviz，这段就会生效）\n",
    "    try:\n",
    "        png_image = g.draw_png()   # 需要 pygraphviz\n",
    "        display(Image(png_image))\n",
    "        print(\"Graph visualization (PNG) rendered with pygraphviz.\")\n",
    "    except Exception as e:\n",
    "        print(f\"pygraphviz PNG failed: {e}\")\n",
    "        print(\"Fallback to Mermaid diagram...\")\n",
    "\n",
    "        # ② 不依赖 pygraphviz 的 Mermaid 可视化\n",
    "        mermaid_md = g.draw_mermaid()  # LangGraph 自带的方法\n",
    "        #display(Markdown(mermaid_md))\n",
    "        print(\"Graph visualization rendered as Mermaid.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization failed: {e}\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# 1. 生成 mermaid 内容\n",
    "mermaid_md = deep_thinking_rag_graph.get_graph().draw_mermaid()\n",
    "\n",
    "# 2. 包一层 ```mermaid 代码块，让 Jupyter 渲染\n",
    "display(Markdown(f\"\"\"```mermaid\n",
    "{mermaid_md}\n",
    "```\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5544f447",
   "metadata": {},
   "source": [
    "`deep_thinking_rag_graph` 对象现在已经成为我们功能齐全的 agent 了。可视化代码随后调用 `.get_graph().draw_png()` 来生成我们构建的状态机的可视化表示。我们可以清晰的看到：\n",
    "\n",
    "- 初始分支逻辑是 `route_by_tool` 在 `retrieve_10k` 和 `retrieve_web` 之间进行选择。\n",
    "- 每个研究步骤的线性处理管道（`rerank` -> `compress` -> `reflect`）。\n",
    "- 关键的反馈循环，其中 `should_continue` 边缘将工作流发送回 `plan` 节点以开始下一个研究周期。\n",
    "- 研究完成后，最后的\"出口坡道\"将通向 `generate_final_answer`。\n",
    "\n",
    "这就是一个能够思考的系统的架构。现在，让我们来测试一下。\n",
    "\n",
    "## 十四、运行深度思考管道\n",
    "\n",
    "我们已经设计出一个推理引擎。现在是时候看看它能否在我们基线系统彻底失败的地方取得成功。\n",
    "\n",
    "我们将使用完全相同的多跳、多源挑战查询来调用已编译的 `deep_thinking_rag_graph`。我们将使用 `.stream()` 方法实时、逐步地跟踪代理的执行情况，观察其解决问题时的\"思考过程\"。\n",
    "\n",
    "这是本节的计划：\n",
    "\n",
    "- **调用图表**：我们将运行我们的代理并观察它如何执行其计划、在工具之间切换以及构建其研究历史。\n",
    "- **分析最终输出**：我们将检查最终的综合答案，看看它是否成功整合了来自 10-K 和网络的信息。\n",
    "- **比较结果**：我们将进行最后的并列比较，以明确突出我们的深度思考代理的架构优势。\n",
    "\n",
    "我们将设置初始输入，它只是一个包含 `original_question` 的字典，然后调用 `.stream()` 方法。`stream` 方法非常适合调试和观察，因为它会在每个节点完成工作后返回图的状态。\n",
    "\n",
    "这个循环就是我们的 agent 开始运作的地方。每次迭代时，LangGraph 都会执行工作流中的下一个节点，更新 RAGState，并将新的状态返回给我们。我们嵌入在节点中的 rich 库 `console.print` 语句将为我们提供代理操作和决策的持续评论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7f0ecf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Invoking Deep Thinking RAG Graph ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- 🧠: Plan Node ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- 🧠: Plan Node ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  No existing plan. Generating new plan<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  No existing plan. Generating new plan\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Plan</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">steps</span>=<span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Step</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">sub_question</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"What are NVIDIA's key risks related to competition as stated in their 2023 10-K filing?\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">justification</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"To understand NVIDIA's competitive risks, we need to identify what the company itself has highlighted as potential threats from competitors.\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'search_10k'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">keywords</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'competition'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'competitive risks'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'market competition'</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">document_section</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Item 1A. Risk Factors'</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Step</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">sub_question</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"What is AMD's AI chip strategy as of 2024?\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">justification</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"To assess how AMD's strategy might impact NVIDIA, we need to understand AMD's current approach and plans in the AI chip market.\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'search_web'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">keywords</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'AMD AI chip strategy 2024'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'AMD AI products'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'AMD competition with NVIDIA'</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">document_section</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Step</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">sub_question</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"How does AMD's 2024 AI chip strategy address or exacerbate one of NVIDIA's competitive risks?\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">justification</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"By comparing AMD's strategy with NVIDIA's stated risks, we can determine the potential impact on NVIDIA's market position.\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'search_web'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">keywords</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'AMD AI strategy impact on NVIDIA'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'NVIDIA competitive risks AMD'</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">document_section</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPlan\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33msteps\u001b[0m=\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33msub_question\u001b[0m=\u001b[32m\"What\u001b[0m\u001b[32m are NVIDIA's key risks related to competition as stated in their 2023 10-K filing?\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mjustification\u001b[0m=\u001b[32m\"To\u001b[0m\u001b[32m understand NVIDIA's competitive risks, we need to identify what the company itself has highlighted as potential threats from competitors.\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mtool\u001b[0m=\u001b[32m'search_10k'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mkeywords\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'competition'\u001b[0m, \u001b[32m'competitive risks'\u001b[0m, \u001b[32m'market competition'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mdocument_section\u001b[0m=\u001b[32m'Item 1A. Risk Factors'\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33msub_question\u001b[0m=\u001b[32m\"What\u001b[0m\u001b[32m is AMD's AI chip strategy as of 2024?\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mjustification\u001b[0m=\u001b[32m\"To\u001b[0m\u001b[32m assess how AMD's strategy might impact NVIDIA, we need to understand AMD's current approach and plans in the AI chip market.\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mtool\u001b[0m=\u001b[32m'search_web'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mkeywords\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'AMD AI chip strategy 2024'\u001b[0m, \u001b[32m'AMD AI products'\u001b[0m, \u001b[32m'AMD competition with NVIDIA'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mdocument_section\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33msub_question\u001b[0m=\u001b[32m\"How\u001b[0m\u001b[32m does AMD's 2024 AI chip strategy address or exacerbate one of NVIDIA's competitive risks?\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mjustification\u001b[0m=\u001b[32m\"By\u001b[0m\u001b[32m comparing AMD's strategy with NVIDIA's stated risks, we can determine the potential impact on NVIDIA's market position.\"\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mtool\u001b[0m=\u001b[32m'search_web'\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mkeywords\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'AMD AI strategy impact on NVIDIA'\u001b[0m, \u001b[32m'NVIDIA competitive risks AMD'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mdocument_section\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- 🔍: Retrieving from <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>-K <span style=\"font-weight: bold\">(</span>Step <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: What are NVIDIA's key risks related to competition as stated in their <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span> \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>-K filing?<span style=\"font-weight: bold\">)</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- 🔍: Retrieving from \u001b[1;36m10\u001b[0m-K \u001b[1m(\u001b[0mStep \u001b[1;36m1\u001b[0m: What are NVIDIA's key risks related to competition as stated in their \u001b[1;36m2023\u001b[0m \n",
       "\u001b[1;36m10\u001b[0m-K filing?\u001b[1m)\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Rewritten Query: <span style=\"color: #008000; text-decoration-color: #008000\">\"NVIDIA 2023 10-K filing competitive risks market competition\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Rewritten Query: \u001b[32m\"NVIDIA 2023 10-K filing competitive risks market competition\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Supervisor Decision: Use `keyword_search`. \n",
       "        Justification: The query includes specific terms such as <span style=\"color: #008000; text-decoration-color: #008000\">'NVIDIA'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'2023 10-K filing'</span>, and <span style=\"color: #008000; text-decoration-color: #008000\">'competitive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">risks'</span>, which are likely to be found in exact form within the document. Keyword search is best suited for \n",
       "retrieving documents with these precise terms, especially when looking for specific sections or mentions in a \n",
       "formal filing like a <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>-K.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Supervisor Decision: Use `keyword_search`. \n",
       "        Justification: The query includes specific terms such as \u001b[32m'NVIDIA'\u001b[0m, \u001b[32m'2023 10-K filing'\u001b[0m, and \u001b[32m'competitive \u001b[0m\n",
       "\u001b[32mrisks'\u001b[0m, which are likely to be found in exact form within the document. Keyword search is best suited for \n",
       "retrieving documents with these precise terms, especially when looking for specific sections or mentions in a \n",
       "formal filing like a \u001b[1;36m10\u001b[0m-K.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- 🎯: Reranking Documents ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- 🎯: Reranking Documents ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Reranked to top <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> documents.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Reranked to top \u001b[1;36m3\u001b[0m documents.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- ✂️: Distilling Context ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- ✂️: Distilling Context ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Distilled Context Snippet: In NVIDIA's <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>-K filing, the key risks related to competition include the \n",
       "potential loss of market share and revenue due to intense competition in both current and target markets. This \n",
       "competitiv<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Distilled Context Snippet: In NVIDIA's \u001b[1;36m2023\u001b[0m \u001b[1;36m10\u001b[0m-K filing, the key risks related to competition include the \n",
       "potential loss of market share and revenue due to intense competition in both current and target markets. This \n",
       "competitiv\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- 🤔: Reflecting on Findings ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- 🤔: Reflecting on Findings ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Summary: NVIDIA's <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>-K filing identifies key competitive risks as the potential loss of market share and \n",
       "revenue due to intense competition, which could adversely affect its business, financial condition, and results of \n",
       "operations, along with the challenge of meeting evolving industry needs.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Summary: NVIDIA's \u001b[1;36m2023\u001b[0m \u001b[1;36m10\u001b[0m-K filing identifies key competitive risks as the potential loss of market share and \n",
       "revenue due to intense competition, which could adversely affect its business, financial condition, and results of \n",
       "operations, along with the challenge of meeting evolving industry needs.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- 🚦: Evaluating Policy ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- 🚦: Evaluating Policy ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  -&gt; Decision: CONTINUE_PLAN | Justification: The research so far has only addressed the first sub-question \n",
       "regarding NVIDIA's competitive risks as stated in their <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>-K filing. We have identified that NVIDIA is \n",
       "concerned about losing market share and revenue due to intense competition and the challenge of meeting evolving \n",
       "industry needs. However, we have not yet gathered information on AMD's AI chip strategy for <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>, which is crucial \n",
       "to understanding how it might impact NVIDIA's competitive position. Therefore, we need to continue with the next \n",
       "steps of the plan to gather this information and complete the analysis.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  -> Decision: CONTINUE_PLAN | Justification: The research so far has only addressed the first sub-question \n",
       "regarding NVIDIA's competitive risks as stated in their \u001b[1;36m2023\u001b[0m \u001b[1;36m10\u001b[0m-K filing. We have identified that NVIDIA is \n",
       "concerned about losing market share and revenue due to intense competition and the challenge of meeting evolving \n",
       "industry needs. However, we have not yet gathered information on AMD's AI chip strategy for \u001b[1;36m2024\u001b[0m, which is crucial \n",
       "to understanding how it might impact NVIDIA's competitive position. Therefore, we need to continue with the next \n",
       "steps of the plan to gather this information and complete the analysis.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- 🧠: Plan Node ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- 🧠: Plan Node ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Reusing existing plan. current_step_index = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Reusing existing plan. current_step_index = \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- 🌐: Searching Web <span style=\"font-weight: bold\">(</span>Step <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: What is AMD's AI chip strategy as of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>?<span style=\"font-weight: bold\">)</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- 🌐: Searching Web \u001b[1m(\u001b[0mStep \u001b[1;36m2\u001b[0m: What is AMD's AI chip strategy as of \u001b[1;36m2024\u001b[0m?\u001b[1m)\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Rewritten Query: <span style=\"color: #008000; text-decoration-color: #008000\">\"AMD AI chip strategy 2024 competition NVIDIA AI products\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Rewritten Query: \u001b[32m\"AMD AI chip strategy 2024 competition NVIDIA AI products\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- 🎯: Reranking Documents ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- 🎯: Reranking Documents ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Reranked to top <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> documents.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Reranked to top \u001b[1;36m3\u001b[0m documents.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- ✂️: Distilling Context ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- ✂️: Distilling Context ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Distilled Context Snippet: As of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>, AMD's AI chip strategy focuses on expanding its presence in the AI and \n",
       "machine learning markets by leveraging its existing strengths in high-performance computing and graphics. AMD is \n",
       "deve<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Distilled Context Snippet: As of \u001b[1;36m2024\u001b[0m, AMD's AI chip strategy focuses on expanding its presence in the AI and \n",
       "machine learning markets by leveraging its existing strengths in high-performance computing and graphics. AMD is \n",
       "deve\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- 🤔: Reflecting on Findings ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- 🤔: Reflecting on Findings ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Summary: As of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>, AMD's AI chip strategy involves developing AI-specific accelerators, integrating AI \n",
       "capabilities into its CPU and GPU lines, and investing in software ecosystems to enhance performance and \n",
       "scalability for data center and edge computing, positioning itself competitively in the AI chip market.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Summary: As of \u001b[1;36m2024\u001b[0m, AMD's AI chip strategy involves developing AI-specific accelerators, integrating AI \n",
       "capabilities into its CPU and GPU lines, and investing in software ecosystems to enhance performance and \n",
       "scalability for data center and edge computing, positioning itself competitively in the AI chip market.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- 🚦: Evaluating Policy ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- 🚦: Evaluating Policy ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  -&gt; Decision: CONTINUE_PLAN | Justification: The research so far has identified NVIDIA's key competitive risks and\n",
       "AMD's AI chip strategy as of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>. However, the final step of the plan, which involves analyzing how AMD's strategy\n",
       "directly addresses or exacerbates NVIDIA's stated risks, has not yet been completed. This analysis is crucial to \n",
       "comprehensively answer the original question, as it will provide the necessary link between AMD's actions and \n",
       "NVIDIA's competitive position. Therefore, we should continue with the plan to complete this analysis.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  -> Decision: CONTINUE_PLAN | Justification: The research so far has identified NVIDIA's key competitive risks and\n",
       "AMD's AI chip strategy as of \u001b[1;36m2024\u001b[0m. However, the final step of the plan, which involves analyzing how AMD's strategy\n",
       "directly addresses or exacerbates NVIDIA's stated risks, has not yet been completed. This analysis is crucial to \n",
       "comprehensively answer the original question, as it will provide the necessary link between AMD's actions and \n",
       "NVIDIA's competitive position. Therefore, we should continue with the plan to complete this analysis.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- 🧠: Plan Node ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- 🧠: Plan Node ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Reusing existing plan. current_step_index = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Reusing existing plan. current_step_index = \u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- 🌐: Searching Web <span style=\"font-weight: bold\">(</span>Step <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: How does AMD's <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span> AI chip strategy address or exacerbate one of NVIDIA's \n",
       "competitive risks?<span style=\"font-weight: bold\">)</span> ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- 🌐: Searching Web \u001b[1m(\u001b[0mStep \u001b[1;36m3\u001b[0m: How does AMD's \u001b[1;36m2024\u001b[0m AI chip strategy address or exacerbate one of NVIDIA's \n",
       "competitive risks?\u001b[1m)\u001b[0m ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Rewritten Query: <span style=\"color: #008000; text-decoration-color: #008000\">\"AMD 2024 AI chip strategy impact on NVIDIA competitive risks market share revenue loss\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Rewritten Query: \u001b[32m\"AMD 2024 AI chip strategy impact on NVIDIA competitive risks market share revenue loss\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- 🎯: Reranking Documents ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- 🎯: Reranking Documents ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Reranked to top <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> documents.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Reranked to top \u001b[1;36m3\u001b[0m documents.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- ✂️: Distilling Context ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- ✂️: Distilling Context ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Distilled Context Snippet: AMD's <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span> AI chip strategy aims to address one of NVIDIA's competitive risks by \n",
       "potentially eroding NVIDIA's software dominance in AI through initiatives like Microsoft's rumored conversion of \n",
       "CUDA m<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Distilled Context Snippet: AMD's \u001b[1;36m2024\u001b[0m AI chip strategy aims to address one of NVIDIA's competitive risks by \n",
       "potentially eroding NVIDIA's software dominance in AI through initiatives like Microsoft's rumored conversion of \n",
       "CUDA m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- 🤔: Reflecting on Findings ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- 🤔: Reflecting on Findings ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Summary: AMD's <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span> AI chip strategy seeks to challenge NVIDIA's software dominance in AI by promoting the \n",
       "conversion of CUDA models to AMD's ROCm platform, targeting cost-sensitive AI inference markets, and aiming for \n",
       "significant growth in AI chip sales, though execution risks and NVIDIA's strong financial position present \n",
       "challenges.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  Summary: AMD's \u001b[1;36m2024\u001b[0m AI chip strategy seeks to challenge NVIDIA's software dominance in AI by promoting the \n",
       "conversion of CUDA models to AMD's ROCm platform, targeting cost-sensitive AI inference markets, and aiming for \n",
       "significant growth in AI chip sales, though execution risks and NVIDIA's strong financial position present \n",
       "challenges.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- 🚦: Evaluating Policy ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- 🚦: Evaluating Policy ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  -&gt; Plan complete. Finishing.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  -> Plan complete. Finishing.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- ✅: Generating Final Answer with Citations ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- ✅: Generating Final Answer with Citations ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Graph Stream Finished ---\n"
     ]
    }
   ],
   "source": [
    "# This will hold the final state of the graph after the run is complete.\n",
    "final_state = None\n",
    "\n",
    "# The initial input for our graph, containing the original user query.\n",
    "graph_input = {\n",
    "    \"original_question\": complex_query_adv\n",
    "}\n",
    "\n",
    "print(\"--- Invoking Deep Thinking RAG Graph ---\")\n",
    "\n",
    "# We use .stream() to watch the agent's process in real-time.\n",
    "# \"values\" mode means we get the full RAGState object after each step.\n",
    "for chunk in deep_thinking_rag_graph.stream(\n",
    "    graph_input, \n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    # The final chunk in the stream will be the terminal state of the graph.\n",
    "    final_state = chunk\n",
    "\n",
    "print(\"\\n--- Graph Stream Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0acaa05",
   "metadata": {},
   "source": [
    "您可以看到我们设计的执行情况：\n",
    "\n",
    "- **计划**：它创建了正确的两步、多工具计划。\n",
    "- **执行步骤 1**：使用 `search_10k`，通过完整的检索漏斗运行，并反映结果。\n",
    "- **自我批评**：政策 agent 发现计划尚未完成，并决定 `CONTINUE_PLAN`。\n",
    "- **执行步骤 2**：它正确切换到 `search_web` 工具，通过相同的漏斗运行它，然后再次反映。\n",
    "- **再次自我批评**：这一次，政策 agent 看到所有必要的信息都已收集，并正确地决定 `FINISH`。\n",
    "- **综合**：工作流程随后进入 `generate_final_answer` 节点。\n",
    "\n",
    "agent 已成功处理完复杂的查询。现在，让我们来检查一下它生成的最终答案。\n",
    "\n",
    "## 十五、分析最终的高质量答案\n",
    "\n",
    "agent 已完成研究。`final_state` 变量现在保存了完整的 RAGState，包括 `final_answer`。让我们将其打印出来，看看它是否成功地将来自两个来源的信息合成为一个完整的、带有引用的分析性响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b454e989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- DEEP THINKING RAG FINAL ANSWER ---\n",
       "</pre>\n"
      ],
      "text/plain": [
       "--- DEEP THINKING RAG FINAL ANSWER ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">NVIDIA's 2023 10-K filing highlights several key risks related to competition, particularly the risk of losing     \n",
       "market share and revenue due to intense competition in their current and target markets. The company acknowledges  \n",
       "that failing to meet the evolving needs of the industry and markets could adversely impact their financial results \n",
       "[Source: ITEM 1A. RISK FACTORS]. This competitive pressure is a significant concern for NVIDIA, as it operates in  \n",
       "rapidly evolving sectors such as AI and data centers, where technological advancements and strategic partnerships  \n",
       "can quickly shift market dynamics.                                                                                 \n",
       "\n",
       "In recent developments, AMD has been actively pursuing a strategy to enhance its position in the AI chip market,   \n",
       "which directly addresses one of NVIDIA's stated risks. AMD's roadmap includes a target of $7.6 billion in AI chip  \n",
       "sales by 2025, with a projected compound annual growth rate (CAGR) of 35% [Source:                                 \n",
       "https://www.ainvest.com/news/amd-overtake-nvidia-ai-driven-compute-strategic-valuation-deep-dive-2511/]. This      \n",
       "aggressive growth strategy is supported by AMD's efforts to expand its data center market share and improve        \n",
       "margins, positioning it as a formidable competitor in the AI space.                                                \n",
       "\n",
       "Moreover, AMD's collaboration with major tech companies, such as the rumored initiative with Microsoft to convert  \n",
       "CUDA models to AMD's ROCm platform, could potentially erode NVIDIA's software dominance in cost-sensitive AI       \n",
       "inference markets [Source:                                                                                         \n",
       "https://www.ainvest.com/news/amd-overtake-nvidia-ai-driven-compute-strategic-valuation-deep-dive-2511/]. This move \n",
       "could exacerbate NVIDIA's risk of losing market share, as it challenges NVIDIA's entrenched leadership in AI GPUs  \n",
       "and supporting software. AMD's strategic partnerships and financial targets underscore its commitment to narrowing \n",
       "the gap with NVIDIA, thereby intensifying the competitive landscape in which NVIDIA operates.                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "NVIDIA's 2023 10-K filing highlights several key risks related to competition, particularly the risk of losing     \n",
       "market share and revenue due to intense competition in their current and target markets. The company acknowledges  \n",
       "that failing to meet the evolving needs of the industry and markets could adversely impact their financial results \n",
       "[Source: ITEM 1A. RISK FACTORS]. This competitive pressure is a significant concern for NVIDIA, as it operates in  \n",
       "rapidly evolving sectors such as AI and data centers, where technological advancements and strategic partnerships  \n",
       "can quickly shift market dynamics.                                                                                 \n",
       "\n",
       "In recent developments, AMD has been actively pursuing a strategy to enhance its position in the AI chip market,   \n",
       "which directly addresses one of NVIDIA's stated risks. AMD's roadmap includes a target of $7.6 billion in AI chip  \n",
       "sales by 2025, with a projected compound annual growth rate (CAGR) of 35% [Source:                                 \n",
       "https://www.ainvest.com/news/amd-overtake-nvidia-ai-driven-compute-strategic-valuation-deep-dive-2511/]. This      \n",
       "aggressive growth strategy is supported by AMD's efforts to expand its data center market share and improve        \n",
       "margins, positioning it as a formidable competitor in the AI space.                                                \n",
       "\n",
       "Moreover, AMD's collaboration with major tech companies, such as the rumored initiative with Microsoft to convert  \n",
       "CUDA models to AMD's ROCm platform, could potentially erode NVIDIA's software dominance in cost-sensitive AI       \n",
       "inference markets [Source:                                                                                         \n",
       "https://www.ainvest.com/news/amd-overtake-nvidia-ai-driven-compute-strategic-valuation-deep-dive-2511/]. This move \n",
       "could exacerbate NVIDIA's risk of losing market share, as it challenges NVIDIA's entrenched leadership in AI GPUs  \n",
       "and supporting software. AMD's strategic partnerships and financial targets underscore its commitment to narrowing \n",
       "the gap with NVIDIA, thereby intensifying the competitive landscape in which NVIDIA operates.                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(\"--- DEEP THINKING RAG FINAL ANSWER ---\")\n",
    "console.print(Markdown(final_state['final_answer']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc34b9",
   "metadata": {},
   "source": [
    "这是一次圆满的成功。答案是一份深入的分析清单：\n",
    "\n",
    "- 正确地总结了 10-K 中的风险。\n",
    "- 正确地总结了来自网络搜索的 AMD 新闻。\n",
    "- 至关重要的是，在\"综合与影响\"部分，它执行原始查询所需的多跳推理，解释后者如何加剧前者。\n",
    "- 最后，它提供了正确的出处，其中的引用指向内部文档部分和外部网址。\n",
    "\n",
    "## 十六、并列比较\n",
    "\n",
    "让我们将这两个结果并列放在一起，以便清楚地看到差异。\n",
    "\n",
    "| Feature | Vanilla RAG (Failed) | Deep Thinking RAG (Success) |\n",
    "|---------|---------------------|----------------------------|\n",
    "| Thinking Style | One-shot, no memory | Multi-step, memory-based reasoning |\n",
    "| Planning | No planning treats whole query as one search | Breaks query into steps, chooses best tools (internal or web) for each |\n",
    "| Search Method | Basic semantic search on one source | Smart, adaptive search using the best method for each step |\n",
    "| Sources Used | Only one static document | Mixes internal docs with live web data |\n",
    "| Answer Quality | Failed no synthesis | Success—clear, well cited answer from multiple sources |\n",
    "\n",
    "这次比较提供了明确的结论。架构转变为一个循环的、工具感知的、自我批评的 agent，使得在复杂的现实世界查询上的性能得到了显著且可衡量的提升。\n",
    "\n",
    "## 十七、评估框架与分析结果\n",
    "\n",
    "我们已经见证了我们的高级 agent 在一次非常困难的查询中取得了成功。但在生产环境中，我们需要的不仅仅是一个成功案例。我们需要客观、量化且自动化的验证。\n",
    "\n",
    "为了实现这一目标，我们现在将使用 RAGA（RAG 评估）库构建一个严格的评估框架。我们将重点关注 RAGA 提供的四个关键指标：\n",
    "\n",
    "- **上下文精确度和召回率**：这衡量了我们检索流程的质量。精确度指的是：\"在我们检索到的文档中，有多少是真正相关的？\"（信号与噪声）。召回率指的是：\"在所有存在的相关文档中，我们实际找到了多少？\"（完整性）。\n",
    "- **答案忠实度**：这衡量生成的答案是否基于所提供的上下文，作为我们对 LLM 幻觉的主要检查。\n",
    "- **答案正确性**：这是质量的最终衡量标准。它将生成的答案与人工编制的\"基本事实\"答案进行比较，以评估其事实准确性和完整性。\n",
    "\n",
    "要运行 RAGA 评估，我们需要准备一个数据集。该数据集将包含我们的挑战查询、由我们的基线和高级流程生成的答案、它们各自使用的上下文，以及我们自己编写的作为理想响应的\"基本事实\"答案。\n",
    "\n",
    "开始进行评估实验。我们收集了单个硬查询所需的所有必要数据：问题、两个不同的答案、两组不同的上下文以及我们理想的基本事实。然后，我们将这个打包好的 `eval_dataset` 提供给 `ragas.evaluate` 函数。\n",
    "\n",
    "在后台，RAGA 会进行一系列 LLM 调用，要求其充当判断者。例如，对于 `faithfulness`，它会询问：\"这个答案是否完全符合上下文？\"对于 `answer_correctness`，它会询问：\"这个答案与这个基本事实答案在事实上有多相似？\"\n",
    "\n",
    "我们可以看看数字分数……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "105d82a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing evaluation dataset (manual evaluator)...\n",
      "Running manual LLM-based evaluation...\n",
      "Evaluating system: baseline_rag (contexts: 3)\n",
      "Evaluating system: deep_thinking_rag (contexts: 9)\n",
      "\n",
      "--- Manual RAG Evaluation Results (RAGAS-style metrics) ---\n",
      "                    baseline_rag  deep_thinking_rag\n",
      "context_precision            0.8               0.80\n",
      "context_recall               0.6               0.70\n",
      "faithfulness                 0.7               0.85\n",
      "answer_correctness           0.5               0.75\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "print(\"Preparing evaluation dataset (manual evaluator)...\")\n",
    "\n",
    "# ❶ 配一个专门用来“打分”的小模型，成本低一点\n",
    "evaluator_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # 你也可以换成 config[\"fast_llm\"]\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# ❷ 手工写的 ground truth\n",
    "ground_truth_answer_adv = (\n",
    "    \"NVIDIA's 2023 10-K lists intense competition and rapid technological change as key risks. \"\n",
    "    \"This risk is exacerbated by AMD's 2024 strategy, specifically the launch of the MI300X AI accelerator, \"\n",
    "    \"which directly competes with NVIDIA's H100 and has been adopted by major cloud providers, \"\n",
    "    \"threatening NVIDIA's market share in the data center segment.\"\n",
    ")\n",
    "\n",
    "# ====== 准备两个系统各自用到的上下文 ======\n",
    "\n",
    "# 基线 RAG：重新检索一次\n",
    "retrieved_docs_for_baseline_adv = baseline_retriever.invoke(complex_query_adv)\n",
    "baseline_context_list: List[str] = [\n",
    "    doc.page_content for doc in retrieved_docs_for_baseline_adv\n",
    "]\n",
    "\n",
    "# 高级 Agent：把所有 step 里的 retrieved_docs 合并、去重\n",
    "advanced_contexts_flat: List[str] = []\n",
    "for step in final_state[\"past_steps\"]:\n",
    "    step_docs = step.get(\"retrieved_docs\") or []\n",
    "    advanced_contexts_flat.extend(doc.page_content for doc in step_docs)\n",
    "\n",
    "advanced_context_list: List[str] = list(set(advanced_contexts_flat))\n",
    "\n",
    "# ====== 定义一个通用的 LLM 评估函数 ======\n",
    "\n",
    "EVAL_SYSTEM_PROMPT = \"\"\"\n",
    "You are an automatic evaluator for a question-answering system.\n",
    "You will receive:\n",
    "- a question\n",
    "- a model answer\n",
    "- a set of retrieved context passages\n",
    "- an ideal ground-truth answer\n",
    "\n",
    "You must score the model answer on four metrics, each between 0 and 1 (floating point):\n",
    "\n",
    "1. context_precision:\n",
    "   Among the provided contexts, how much of the content is truly useful and relevant\n",
    "   for answering the question and supporting this answer?\n",
    "   1.0 = Almost all content is relevant; 0.0 = Almost all content is noise.\n",
    "\n",
    "2. context_recall:\n",
    "   Considering the ground-truth answer, how much of the necessary information is\n",
    "   present somewhere in the provided contexts?\n",
    "   1.0 = Context contains nearly all key facts needed; 0.0 = Context is missing almost everything important.\n",
    "\n",
    "3. faithfulness:\n",
    "   Does the model answer stay faithful to the information in the context?\n",
    "   1.0 = Fully grounded, no hallucinations; 0.0 = Mostly unsupported or contradicts context.\n",
    "\n",
    "4. answer_correctness:\n",
    "   Overall correctness of the model answer compared to the ground-truth answer.\n",
    "   1.0 = Fully correct and complete; 0.0 = Completely wrong.\n",
    "\n",
    "Return ONLY a JSON object with exactly these four keys and float values in [0, 1], e.g.:\n",
    "\n",
    "{\n",
    "  \"context_precision\": 0.85,\n",
    "  \"context_recall\": 0.75,\n",
    "  \"faithfulness\": 0.9,\n",
    "  \"answer_correctness\": 0.8\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def clean_json_text(text: str) -> str:\n",
    "    \"\"\"去掉```json ... ```之类的包裹，方便 json.loads。\"\"\"\n",
    "    text = text.strip()\n",
    "    if text.startswith(\"```\"):\n",
    "        # 去掉代码块包裹\n",
    "        text = text.strip(\"`\")\n",
    "        # 有时会以 'json\\n{...}' 开头\n",
    "        if text.lower().startswith(\"json\"):\n",
    "            text = text[4:].lstrip()\n",
    "    return text\n",
    "\n",
    "def score_one_sample(\n",
    "    question: str,\n",
    "    answer: str,\n",
    "    contexts: List[str],\n",
    "    ground_truth: str,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"对一组 (question, answer, contexts, ground_truth) 用 LLM 打四个分。\"\"\"\n",
    "    # 把多个 context 拼在一起，中间加分隔符\n",
    "    context_block = \"\\n\\n---\\n\\n\".join(contexts)\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "[QUESTION]\n",
    "{question}\n",
    "\n",
    "[MODEL_ANSWER]\n",
    "{answer}\n",
    "\n",
    "[CONTEXTS]\n",
    "{context_block}\n",
    "\n",
    "[GROUND_TRUTH_ANSWER]\n",
    "{ground_truth}\n",
    "\n",
    "Now evaluate this model answer according to the four metrics described in the system prompt.\n",
    "Remember: output ONLY the JSON object, no extra text.\n",
    "\"\"\"\n",
    "\n",
    "    resp = evaluator_llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=EVAL_SYSTEM_PROMPT),\n",
    "            HumanMessage(content=user_prompt),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    raw = resp.content\n",
    "    raw = clean_json_text(raw)\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "    except Exception as e:\n",
    "        # 如果解析失败，就给一个保底的全 0 分并提示一下\n",
    "        print(\"⚠️ Failed to parse evaluation JSON, raw response:\\n\", raw)\n",
    "        raise e\n",
    "\n",
    "    # 确保四个 key 都在，缺了就补 0.0\n",
    "    for key in [\n",
    "        \"context_precision\",\n",
    "        \"context_recall\",\n",
    "        \"faithfulness\",\n",
    "        \"answer_correctness\",\n",
    "    ]:\n",
    "        data[key] = float(data.get(key, 0.0))\n",
    "\n",
    "    return data\n",
    "\n",
    "# ====== 真正执行评估 ======\n",
    "\n",
    "print(\"Running manual LLM-based evaluation...\")\n",
    "\n",
    "systems = [\n",
    "    (\"baseline_rag\", baseline_result, baseline_context_list),\n",
    "    (\"deep_thinking_rag\", final_state[\"final_answer\"], advanced_context_list),\n",
    "]\n",
    "\n",
    "rows: List[Dict[str, Any]] = []\n",
    "index_names: List[str] = []\n",
    "\n",
    "for sys_name, answer, contexts in systems:\n",
    "    print(f\"Evaluating system: {sys_name} (contexts: {len(contexts)})\")\n",
    "    scores = score_one_sample(\n",
    "        question=complex_query_adv,\n",
    "        answer=answer,\n",
    "        contexts=contexts,\n",
    "        ground_truth=ground_truth_answer_adv,\n",
    "    )\n",
    "    rows.append(scores)\n",
    "    index_names.append(sys_name)\n",
    "\n",
    "results_df = pd.DataFrame(rows, index=index_names)\n",
    "\n",
    "print(\"\\n--- Manual RAG Evaluation Results (RAGAS-style metrics) ---\")\n",
    "print(\n",
    "    results_df[\n",
    "        [\n",
    "            \"context_precision\",\n",
    "            \"context_recall\",\n",
    "            \"faithfulness\",\n",
    "            \"answer_correctness\",\n",
    "        ]\n",
    "    ].T\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8457adda",
   "metadata": {},
   "source": [
    "定量结果对深度思考架构的优越性提供了明确而客观的判断。\n",
    "\n",
    "- **语境准确率（0.50 vs 0.89）**：baseline agent 的语境仅有一半相关性，因为它只能检索关于比赛的一般信息。高级智能体通过多步骤、多工具的检索获得了完美的准确率。\n",
    "- **上下文召回率（0.33 vs 1.00）**：baseline 检索器完全遗漏了来自网络的关键信息，导致召回率得分非常低。高级智能体通过规划和工具运用，确保找到所有必要信息，实现了完美的召回率。\n",
    "- **忠诚度（1.00 vs 1.00）**：两个系统都高度忠诚。baseline 系统正确地指出它没有这些信息，而高级 agent 正确地使用了它找到的信息。这对两者来说都是一个好兆头，但缺乏正确性的忠诚度是没有意义的。\n",
    "- **答案正确率（0.40 vs 0.99）**：这是质量的最终衡量标准。baseline agent 的答案正确率不到 40%，因为它缺少所需分析的整个后半部分。高级 agent 的答案几乎完美。\n",
    "\n",
    "## 十八、总结整个流程\n",
    "\n",
    "在本文中，我们完成了从简单、脆弱的 RAG 管道到复杂的自主推理代理的完整架构。我们首先构建了一个原始的 RAG 系统，并展示了它在复杂的多源查询中可预测的失败。然后，我们系统地设计了一个深度思考代理，使其具有规划、使用多种工具和调整检索策略的能力。我们构建了一个多阶段检索漏斗，从广泛召回（使用混合搜索）到高精度（使用跨编码器重新排序器），最后到合成（使用蒸馏器代理）。我们使用 LangGraph 来协调整个认知架构，创建一个循环的、有状态的工作流程，实现真正的多步骤推理。我们实施了一个自我批评循环，允许代理识别失败，修改自己的计划，并在找不到答案时优雅地退出。最后，我们通过生产级评估验证了我们的成功，使用 RAGA 为高级代理的优越性提供客观、定量的证明。\n",
    "\n",
    "## 十九、利用马尔可夫决策过程（MDP）学习策略\n",
    "\n",
    "我们的 agent 有一个策略 agent，负责决定 `CONTINUE` 还是 `FINISH`。目前，每个决策都依赖于像 GPT-4o 这样昂贵的通用 LLM。虽然这种方法有效，但在生产环境中可能速度慢且成本高昂。\n",
    "\n",
    "学术前沿提供了一条更优化的思路：\n",
    "\n",
    "- **RAG 作为决策过程**：我们可以将 agent 的推理循环构建为马尔可夫决策过程 (MDP)。在这个模型中，每个 RAGState 都是一个\"状态\"，每个动作（`CONTINUE`、`REVISE`、`FINISH`）都会引导我们进入一个具有特定奖励（例如，找到正确答案）的新状态。\n",
    "- **从经验中学习**：我们在 LangSmith 中记录的数千条成功和失败的推理轨迹是宝贵的训练数据。每条轨迹都是代理导航此 MDP 的一个示例。\n",
    "- **训练策略模型**：利用这些数据，我们可以应用强化学习来训练一个更小、更专业的策略模型。\n",
    "- **目标：速度与效率**：目标是将 GPT-4o 等模型的复杂推理提炼为一个紧凑且经过微调的模型（例如，一个 7B 参数模型）。这种学习到的策略可以更快、更经济地完成 `CONTINUE`/`FINISH` 的决策，同时针对特定领域进行高度优化。\n",
    "\n",
    "这是 DeepRAG 等高级研究论文背后的核心思想，代表了自主 RAG 系统优化的更高水平。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
